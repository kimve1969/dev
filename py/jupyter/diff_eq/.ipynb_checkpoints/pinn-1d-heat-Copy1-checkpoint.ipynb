{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9a4b21-d8d9-46af-bf6f-77c1184a046a",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "Теоретическая основа: универсальная теорема аппроксимации - теорема Хорника-Цирельсона-Уайта: \\\n",
    "Однослойная нейроная сеть с достаточно большим числом нейронов и нелинейной гладкой функцией \\\n",
    "(например, sigmoid или tanh) может приблизить любую непрерывную функцию на компактном множестве \\\n",
    "с любой заданной точностью. \n",
    "\n",
    "$$\n",
    "sup_{x \\in K}| f(x) - N(x) | < \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0bfc7-18fc-4b56-8e59-37cba3dcd992",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Уравнение теплопроводности\n",
    "\n",
    "$$ \\frac{\\partial u(x,t)}{\\partial t}=\\frac{\\partial}{\\partial x}(k(x)*\\frac{\\partial u(x,t)}{\\partial x})+f(x,t)) $$\n",
    "$$ x \\in (0,L), \\, t \\in (0,T] $$ \\\n",
    "$ u(x,t) $ - температура  \\\n",
    "$ k(x) $ - коэф.теплопроводности \\\n",
    "$ f(x,t) $ - источники \\\\ стоки\n",
    "### Начальные условия:\n",
    "$$ u(x,0)=g_{IC}(x) $$\n",
    "$$ x \\in [0,L], \\, t = 0 $$\n",
    "### Граничные условия (например, первого рода):\n",
    "$$ u(0,t)=g_{LB}(t), u(L,t)=g_{RB}(t) $$\n",
    "$$ x \\in \\{0,L\\}, \\, t \\in (0,T] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ae47f-ef7b-4e7b-9b07-5a7d6f99e630",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Аппроксимация дифференциального уравнения нейроной сетью\n",
    "$ N_u(x,t) $ - нейросеть аппроксимирует u(x,t),тогда аппроксимированное ур-ие теплопроводности примет вид:\n",
    "$$\n",
    "\\frac{\\partial N_u(x,t)}{\\partial t}=\\frac{\\partial}{\\partial x}(k(x) \\frac{\\partial N_u(x,t)}{\\partial x})+f(x,t))\n",
    "$$\n",
    "\n",
    "или раскроем производную произведения и перенесем все в левую сторону\n",
    "$$\n",
    "\\frac{\\partial N_u(x,t)}{\\partial t} - \\frac{\\partial k(x)}{\\partial x} \\frac{\\partial N_u(x,t)}{\\partial x} - k(x) \\frac{\\partial^2 N_u(x,t)}{\\partial x^2} -f(x,t)) = 0\n",
    "$$\n",
    "\n",
    "Производная нейросети $ N_u(x,t) $, состоящая, например, из входа -> два внутренних слоя -> выход - \n",
    "это гладкая функция как композиция слоев нейросети \\\n",
    "$$ N_u(x,t) = f_3(f_2(f_1(x,t,\\theta), \\theta), \\theta), $$ \\\n",
    "$ f_1, f_2, f_3 $ - слои нейросети (линейные преобразования + функции активации) \\\n",
    "$ \\theta_i $ - обучаемые параметры (веса и смещения) \\\n",
    "производная $  N_u(x,t) $ вычисляется по цепному правилу:\n",
    "$$\n",
    "\\frac{\\partial N_u}{\\partial x} = \\frac{\\partial f_3}{\\partial f_2} * \\frac{\\partial f_2}{\\partial f_1} * \\frac{\\partial f_1}{\\partial x}\n",
    "$$\n",
    "где каждый член это якобиан (матрица частных производных)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb32d2a-6e34-4abd-8f53-2740e7ab5440",
   "metadata": {},
   "source": [
    "# Функция потерь (loss function)\n",
    "### 1. Partial Differential Equation loss: ошибка аппроксимации дифф.ур-ия нейронной сетью\n",
    "$$\n",
    "L_{PDE} = \\frac{1}{M} \\sum_{j=1}^M || \\frac{\\partial N_u(x,t)}{\\partial t} - \\frac{\\partial k(x)}{\\partial x} \\frac{\\partial N_u(x,t)}{\\partial x} - k(x) \\frac{\\partial^2 N_u(x,t)}{\\partial x^2} -f(x,t) ||^2\n",
    "$$\n",
    "$$ x \\in (0,L), t \\in (0,T] $$\n",
    "### 2. Initial condition loss: ошибка нейронной сети по начальным условиям\n",
    "$$ L_{IC} = \\frac{1}{K} \\sum_{j=1}^K || N_u(x,0) - g_{IC}(x) ||^2 $$\n",
    "$$ x \\in [0;L], t = 0 $$\n",
    "### 3. Boundary condition loss: ошибка нейронной сети по граничным условиям\n",
    "$$ L_{BC} = \\frac{1}{P} (\\sum_{j=1}^P || N_u(0,t) - g_{LB}(t) ||^2 + \\sum_{j=1}^P || N_u(L,t) - g_{RB}(t) ||^2) $$\n",
    "$$ x \\in \\{0,L\\}, t \\in (0,T] $$\n",
    "### 4. Data loss: ошибка предсказанных и экспериментальных данных\n",
    "$$ L_{data} = \\frac{1}{N} \\sum_{j=1}^N || N_u(x,t) - u_{data}(x,t) ||^2 $$\n",
    "$$ x \\in (0,L), t \\in (0,T] $$\n",
    "### Итоговый лосс:\n",
    "$$ L = \\lambda_{PDE} L_{PDE} + \\lambda_{IC} L_{IC} + \\lambda_{BC} L_{BC} + \\lambda_{data} L_{data} $$\n",
    "пояснение по выбору M,K,P,N пусть \n",
    "K - число пропорциональное кол-ву шагов дискретизации по времени, \n",
    "P - число пропорциональное кол-ву шагов дискретизации по пространству (в данном случае одномерному)\n",
    "тогда \n",
    "M - будет числом пропорциональным K*P кол-ву шагов дискретизации внутренней области определения\n",
    "N - кол-во возможных экспериментальных данных, которые попадают во внутреннюю область \\\n",
    "Таким образом:\n",
    "1. Решаем ПРЯМУЮ ЗАДАЧУ - на выходе получаем PINN физической модели (аппроксимацию дифференциального уравнения в частных производных с учетом нач. и гр. условий)\n",
    "$$ L_{phys} = \\lambda_{PDE} L_{PDE} + \\lambda_{IC} L_{IC} + \\lambda_{BC} L_{BC} $$\n",
    "3. Решаем ОБРАТНУЮ ЗАДАЧУ - дополнительно к аппроксимации физической модели $ N_u(x,t) $ добавляем в уравнение аппроксимацию, например, коэффициента теплопроводности $ N_k(x) $ и решаем задачу оптимизации двух PINN сетей  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef0923-b2bc-46b2-9468-0dd0a2b254e5",
   "metadata": {},
   "source": [
    "Пояснение по виду лоссов: $ loss = \\frac{1}{N} \\sum_{j=1}^N ||x||^2  $, где ||*|| - Евклидова норма. \\\n",
    "Если x - вектор независимых параметров, т.е. $ x \\in \\{x_1, x_2, ..., x_n\\} $\n",
    "то $ ||x|| = \\sqrt {x_1^2 + x_2^2 + ... x_n^2} $ (длина n-мерного вектора)  или $ ||x||^2 = \\sum_{i=1}^n x_i^2 $, тогда:\n",
    "$ loss =  \\frac{1}{N} \\sum_{j=1}^N (\\sum_{i=1}^n x_i^2) $, где первая сумма (справа) - длина n-мерного вектора в квадрате,\n",
    "а вторая сумма (слева) - среднее значение N длин в квадрате n-мерных векторов \\\n",
    "Если x - это скаляр, тогда: $ loss = \\frac{1}{N} \\sum_{j=1}^N (x^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439210de-c544-419e-80c6-99204b2d919e",
   "metadata": {},
   "source": [
    "# ПРИМЕР\n",
    "## 1. ПРЯМАЯ ЗАДАЧА\n",
    "### Пусть исходные данные следующие:\n",
    "$$ x \\in [0,1], \\, t \\in [0,1] $$\n",
    "$$ u(0,t) = u(L,t)=0 $$\n",
    "$$ u(x,0) = sin(\\pi * x) $$\n",
    "$$ k(x) = k_a \\, cos(2 \\pi x) + k_b $$ \n",
    "$$ \\frac{\\partial k(x)}{\\partial x} = - k_a 2 \\pi \\, sin(2 \\pi x) $$\n",
    "$$ f(x,t) = 0 $$\n",
    "Значения $ k_a $ и $ k_b $ такие, что $ k(x) $ положителен на всем интервале x тогда:\n",
    "### Partial Differential Equation loss:\n",
    "$$\n",
    "L_{PDE} = \\frac{1}{M} \\sum_{j=1}^M || \\frac{\\partial N_u(x,t)}{\\partial t} + k_a 2 \\pi \\, sin(2 \\pi x)*\\frac{\\partial N_u(x,t)}{\\partial x} - (k_a \\, cos(2 \\pi x) + k_b) *\\frac{\\partial^2 N_u(x,t)}{\\partial x^2} ||^2\n",
    "$$ \n",
    "$$ x \\in (0;L), t \\in (0;T] $$\n",
    "### Initial condition loss:\n",
    "$$ L_{IC} = \\frac{1}{K} \\sum_{j=1}^K || N_u(x,0) - sin(\\pi * x) ||^2 $$\n",
    "$$ x \\in [0;L], t = 0 $$\n",
    "### Boundary condition loss:\n",
    "$$ L_{BC} = \\frac{1}{P} (\\sum_{j=1}^P || N_u(0,t) ||^2 + \\sum_{j=1}^P || N_u(L,t) ||^2) $$\n",
    "$$ x=\\{0,L\\}, t \\in (0;T] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530f509-3f15-4a3d-9bae-797cfe25ea1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1.1 Создаем PDE нейроную сеть\n",
    "### 2 (вход) -> 5 (1-ый внутр.слой) -> 5 (2-ой внутр.слой) -> 1 (выход)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1400555e-e269-4529-ba74-bd61586d0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем нейросеть 2->5->5->1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Nu( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2,5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.fc( torch.cat([x, t], dim=1) ) # self.layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c315ad1d-bb6e-4542-b529-419f5bd05e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nu(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net = Nu()\n",
    "u_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357a30b-c37f-46d1-9940-6f7202b998b7",
   "metadata": {},
   "source": [
    "# 1.2 Проверка работы нейросети (вручную проходим от входа на выход по всем слоям)\n",
    "### 1.2.1 2 нейрона (вход) -> 5 нейронов (1-ый внутр.слой)\n",
    "$$\n",
    "h_i = Sigmoid(\\sum_{j=1}^2 w_{ij}x_j + b_i), i = 1,2,3,4,5\n",
    "$$\n",
    "\n",
    "или в матричном кратком виде: $ h = Sigmoid(W*x+b) $, или в матричном полном виде:\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "h_4 \\\\\n",
    "h_5\n",
    "\\end{vmatrix} \\quad = \\quad\n",
    "Sigmoid\n",
    "\\begin{pmatrix}\n",
    "\\begin{vmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32} \\\\\n",
    "w_{41} & w_{42} \\\\\n",
    "w_{51} & w_{52}\n",
    "\\end{vmatrix} *\n",
    "\\begin{vmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{vmatrix} \\quad + \\quad\n",
    "\\begin{vmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4 \\\\\n",
    "b_5\n",
    "\\end{vmatrix}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a124f725-7fef-439d-b570-8a853a501c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.2000]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = torch.tensor([0.1, 0.2]).view(-1,1)\n",
    "xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "59e928a0-341e-477c-b160-0780c511a2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2971, -0.1073],\n",
      "        [ 0.4041, -0.2996],\n",
      "        [ 0.2805,  0.5935],\n",
      "        [ 0.4604,  0.0734],\n",
      "        [ 0.3943, -0.4974]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3398,  0.2451, -0.2366, -0.5725, -0.1150], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2-> 5\n",
    "print(u_net.fc[0].weight)\n",
    "print(u_net.fc[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f9d897e-0155-4e36-942d-b3cfe6c99c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5862],\n",
       "        [0.5562],\n",
       "        [0.4776],\n",
       "        [0.3748],\n",
       "        [0.4564]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h = Sigmoid( W(1)x+b(1) )\n",
    "h = torch.sigmoid( u_net.fc[0].weight @ xt + u_net.fc[0].bias.view(-1,1) )\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016ae05-5944-438b-805a-86e530446006",
   "metadata": {},
   "source": [
    "### 1.2.2 5 нейронов (1-ый внутр.слой) -> 5 нейронов (2-ый внутр.слой)\n",
    "$$\n",
    "g_i = Sigmoid(\\sum_{j=1}^5 w_{ij}h_j + b_i), i = 1,2,3,4,5\n",
    "$$\n",
    "\n",
    "или в матричном кратком виде: $ g = Sigmoid(W*h+b) $, или в матричном полном виде:\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "g_1 \\\\\n",
    "g_2 \\\\\n",
    "g_3 \\\\\n",
    "g_4 \\\\\n",
    "g_5\n",
    "\\end{vmatrix} \\quad = \\quad\n",
    "Sigmoid\n",
    "\\begin{pmatrix}\n",
    "\\begin{vmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} & w_{15} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} & w_{25} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34} & w_{35} \\\\\n",
    "w_{41} & w_{42} & w_{43} & w_{44} & w_{45} \\\\\n",
    "w_{51} & w_{52} & w_{53} & w_{54} & w_{55} \n",
    "\\end{vmatrix} *\n",
    "\\begin{vmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "h_4 \\\\\n",
    "h_5\n",
    "\\end{vmatrix} \\quad + \\quad\n",
    "\\begin{vmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4 \\\\\n",
    "b_5\n",
    "\\end{vmatrix}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "531df2ed-c8ea-4cf6-9e2c-ba50ed65f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3868, -0.0568, -0.0261,  0.2307,  0.1421],\n",
      "        [ 0.1288, -0.1486, -0.4219, -0.2583, -0.4213],\n",
      "        [-0.4125,  0.2083,  0.1472, -0.2239, -0.2070],\n",
      "        [ 0.1134,  0.2205, -0.1971, -0.3777, -0.1586],\n",
      "        [ 0.1446,  0.4001, -0.1700, -0.3297, -0.2827]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3862, -0.3407, -0.1419, -0.3057,  0.2879], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 5-> 5\n",
    "print(u_net.fc[2].weight)\n",
    "print(u_net.fc[2].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "064fbc0d-b338-426d-a6b8-c35ea635bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3762],\n",
       "        [0.3019],\n",
       "        [0.4071],\n",
       "        [0.3954],\n",
       "        [0.5650]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g = Sigmoid( W(2)h+b(2) )\n",
    "g = torch.sigmoid( u_net.fc[2].weight @ h + u_net.fc[2].bias.view(-1,1) )\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a980c-b802-43f4-9565-60cd23f43467",
   "metadata": {},
   "source": [
    "### 1.2.3 5 нейронов (2-ый внутр.слой) -> 1 нейрон (выход)\n",
    "$$\n",
    "y_i = \\sum_{j=1}^5 w_{ij}g_j + b_i, i = 1\n",
    "$$\n",
    "\n",
    "или в матричном кратком виде: $ y = W*g+b $, или в матричном полном виде:\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "y_1\n",
    "\\end{vmatrix} \\quad = \\quad\n",
    "\\begin{vmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} & w_{15} \\\\\n",
    "\\end{vmatrix} *\n",
    "\\begin{vmatrix}\n",
    "g_1 \\\\\n",
    "g_2 \\\\\n",
    "g_3 \\\\\n",
    "g_4 \\\\\n",
    "g_5\n",
    "\\end{vmatrix} \\quad + \\quad\n",
    "\\begin{vmatrix}\n",
    "b_1\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Обратите внимание, что функции активации нет, но могла бы быть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f2edd52-981f-454a-9946-a696514d0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3285, -0.2794, -0.0612, -0.3419, -0.0533]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4354], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 5 -> 1\n",
    "print(u_net.fc[4].weight)\n",
    "print(u_net.fc[4].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9f51531-2720-4965-a351-2960919620f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5864]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = W(3)g+b(3)\n",
    "y = u_net.fc[4].weight @ g + u_net.fc[4].bias.view(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94d001-7036-42db-8e5a-950c88e9539c",
   "metadata": {},
   "source": [
    "# 1.3 Проверка ручных вычислений с автоматическим (передачей параметров в нейроную сеть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9236b81-59f9-4edb-8fdb-717b29ea9bb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.2000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5864]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# придется входные данные представить немного в ином виде, т.к. в модели нейронки Nu параметры передаются\n",
    "# в пакетном batch режиме\n",
    "x_batch = xt[0].view(-1,1)\n",
    "t_batch = xt[1].view(-1,1)\n",
    "print(torch.cat([x_batch,t_batch], dim=1))\n",
    "# предсказания, на данном этапе нейронка инициализируется каким-то начальными значениями, \n",
    "# соответсвенно предсказание носит некий случайный характер в начале\n",
    "u_pred = u_net(x_batch,t_batch)\n",
    "u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6996b80-5642-444f-9231-93856d75fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if y == u_pred:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a077f-4f7e-4e66-a1f5-e06f7e83575d",
   "metadata": {},
   "source": [
    "# 1.4 Задаем области определения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec53b7-c5cd-4a11-a9cb-c6995eb28a08",
   "metadata": {},
   "source": [
    "Пусть вся область будет 10х10, из них:\n",
    "1. внутрення область - 8х9 = 72\n",
    "2. начальные условия - 10х1 = 10\n",
    "3. граничные условия - 9х1х2 = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1785e5-d230-461b-b2c6-e472adbd2b74",
   "metadata": {},
   "source": [
    "### $ x \\in (0,1) \\, , t \\in (0,1] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6898a1c5-5635-4d73-bb3c-8898cc3fcf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.linspace(0,1,10)[1:9]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12af9f8-23b7-49bd-9836-92aea5c6fc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.linspace(0,1,10)[1:9]\n",
    "x_batch_pde = torch.empty(0)\n",
    "\n",
    "for x in temp:\n",
    "    x_batch_pde = torch.cat( [x_batch_pde, x * torch.ones(9) ])\n",
    "x_batch_pde = x_batch_pde.view(-1,1)\n",
    "x_batch_pde.requires_grad = True\n",
    "x_batch_pde.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142f040e-b2e3-45ff-ab63-3833c30905d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_batch_pde = torch.empty(0)\n",
    "\n",
    "for i in temp:\n",
    "    t_batch_pde = torch.cat( [ t_batch_pde, torch.linspace(0,1,10)[1:10] ] )\n",
    "t_batch_pde = t_batch_pde.view(-1,1)\n",
    "t_batch_pde.requires_grad = True\n",
    "t_batch_pde.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b26525b-e42e-413d-a973-0968dec87dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat( [ x_batch_pde, t_batch_pde], dim=1 ).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026b1fe-78ad-4dd2-825d-0554d711430b",
   "metadata": {},
   "source": [
    "### $ x \\in [0,1] \\, , t=0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcee84d-36a4-406e-9c99-a385325d77ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch_ic = torch.linspace(0,1,10).view(-1,1)\n",
    "t_batch_ic = torch.zeros(10).view(-1,1)\n",
    "\n",
    "x_batch_ic.requires_grad = True\n",
    "t_batch_ic.requires_grad = True\n",
    "\n",
    "torch.cat([x_batch_ic, t_batch_ic], dim=1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726ee59-c60e-4def-b9e5-35895e8ed851",
   "metadata": {},
   "source": [
    "### $ x \\in \\{0,1\\} \\, , t \\in (0,1] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93efde92-c456-423e-9349-025b52eb988b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch_bc = torch.cat( [torch.zeros(9).view(-1,1), torch.ones(9).view(-1,1)])\n",
    "t_batch_bc = torch.cat( [torch.linspace(0,1,10)[1:10].view(-1,1), torch.linspace(0,1,10)[1:10].view(-1,1)] )\n",
    "\n",
    "x_batch_bc.requires_grad = True\n",
    "t_batch_bc.requires_grad = True\n",
    "\n",
    "torch.cat([x_batch_bc, t_batch_bc], dim=1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c377a-6b8b-41ff-a51a-0a5852708a4b",
   "metadata": {},
   "source": [
    "### Полная область определения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bd0de0-5a3f-4862-b613-9caa3a757e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = torch.cat([x_batch_pde, x_batch_ic, x_batch_bc])\n",
    "t_batch = torch.cat([t_batch_pde, t_batch_ic, t_batch_bc])\n",
    "\n",
    "torch.cat([x_batch, t_batch], dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0052955e-17f2-48a2-8655-0289a2bb8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net = Nu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a49192b-da82-4586-9c28-8633655bbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam( u_net.parameters(), lr=1e-2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f25709-91d1-489f-9c9d-f6ea45a0d88f",
   "metadata": {},
   "source": [
    "# 1.5 Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2360e264-e07d-42ee-9ee2-4eacd355045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimve/.local/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], Loss: 0.28937, loss pde: 0.00001, loss ic: 0.22296, loss bc: 0.06640\n",
      "Epoch [20/10000], Loss: 0.28341, loss pde: 0.00040, loss ic: 0.20150, loss bc: 0.08150\n",
      "Epoch [40/10000], Loss: 0.27087, loss pde: 0.00775, loss ic: 0.19424, loss bc: 0.06887\n",
      "Epoch [60/10000], Loss: 0.26773, loss pde: 0.01926, loss ic: 0.18684, loss bc: 0.06163\n",
      "Epoch [80/10000], Loss: 0.26425, loss pde: 0.01485, loss ic: 0.18690, loss bc: 0.06249\n",
      "Epoch [100/10000], Loss: 0.25429, loss pde: 0.01307, loss ic: 0.17932, loss bc: 0.06189\n",
      "Epoch [120/10000], Loss: 0.23958, loss pde: 0.00692, loss ic: 0.16519, loss bc: 0.06747\n",
      "Epoch [140/10000], Loss: 0.22869, loss pde: 0.00517, loss ic: 0.15575, loss bc: 0.06777\n",
      "Epoch [160/10000], Loss: 0.20614, loss pde: 0.00132, loss ic: 0.14491, loss bc: 0.05992\n",
      "Epoch [180/10000], Loss: 0.16566, loss pde: 0.00001, loss ic: 0.12009, loss bc: 0.04556\n",
      "Epoch [200/10000], Loss: 0.11160, loss pde: 0.00001, loss ic: 0.08239, loss bc: 0.02920\n",
      "Epoch [220/10000], Loss: 0.05553, loss pde: 0.00000, loss ic: 0.03698, loss bc: 0.01855\n",
      "Epoch [240/10000], Loss: 0.02237, loss pde: 0.00000, loss ic: 0.01338, loss bc: 0.00900\n",
      "Epoch [260/10000], Loss: 0.01298, loss pde: 0.00040, loss ic: 0.00850, loss bc: 0.00408\n",
      "Epoch [280/10000], Loss: 0.00965, loss pde: 0.00013, loss ic: 0.00748, loss bc: 0.00203\n",
      "Epoch [300/10000], Loss: 0.00792, loss pde: 0.00000, loss ic: 0.00614, loss bc: 0.00178\n",
      "Epoch [320/10000], Loss: 0.00687, loss pde: 0.00000, loss ic: 0.00515, loss bc: 0.00172\n",
      "Epoch [340/10000], Loss: 0.00612, loss pde: 0.00000, loss ic: 0.00448, loss bc: 0.00164\n",
      "Epoch [360/10000], Loss: 0.00551, loss pde: 0.00000, loss ic: 0.00396, loss bc: 0.00155\n",
      "Epoch [380/10000], Loss: 0.00498, loss pde: 0.00000, loss ic: 0.00355, loss bc: 0.00143\n",
      "Epoch [400/10000], Loss: 0.00450, loss pde: 0.00000, loss ic: 0.00321, loss bc: 0.00129\n",
      "Epoch [420/10000], Loss: 0.00405, loss pde: 0.00000, loss ic: 0.00290, loss bc: 0.00115\n",
      "Epoch [440/10000], Loss: 0.00364, loss pde: 0.00000, loss ic: 0.00263, loss bc: 0.00101\n",
      "Epoch [460/10000], Loss: 0.00326, loss pde: 0.00000, loss ic: 0.00238, loss bc: 0.00087\n",
      "Epoch [480/10000], Loss: 0.00290, loss pde: 0.00000, loss ic: 0.00215, loss bc: 0.00075\n",
      "Epoch [500/10000], Loss: 0.00258, loss pde: 0.00000, loss ic: 0.00194, loss bc: 0.00064\n",
      "Epoch [520/10000], Loss: 0.00229, loss pde: 0.00000, loss ic: 0.00174, loss bc: 0.00055\n",
      "Epoch [540/10000], Loss: 0.00202, loss pde: 0.00000, loss ic: 0.00155, loss bc: 0.00047\n",
      "Epoch [560/10000], Loss: 0.00179, loss pde: 0.00000, loss ic: 0.00138, loss bc: 0.00040\n",
      "Epoch [580/10000], Loss: 0.00158, loss pde: 0.00000, loss ic: 0.00123, loss bc: 0.00035\n",
      "Epoch [600/10000], Loss: 0.00139, loss pde: 0.00000, loss ic: 0.00109, loss bc: 0.00030\n",
      "Epoch [620/10000], Loss: 0.00123, loss pde: 0.00000, loss ic: 0.00097, loss bc: 0.00026\n",
      "Epoch [640/10000], Loss: 0.00110, loss pde: 0.00000, loss ic: 0.00087, loss bc: 0.00022\n",
      "Epoch [660/10000], Loss: 0.00098, loss pde: 0.00000, loss ic: 0.00078, loss bc: 0.00020\n",
      "Epoch [680/10000], Loss: 0.00088, loss pde: 0.00000, loss ic: 0.00071, loss bc: 0.00017\n",
      "Epoch [700/10000], Loss: 0.00087, loss pde: 0.00007, loss ic: 0.00065, loss bc: 0.00015\n",
      "Epoch [720/10000], Loss: 0.00078, loss pde: 0.00004, loss ic: 0.00060, loss bc: 0.00013\n",
      "Epoch [740/10000], Loss: 0.00068, loss pde: 0.00000, loss ic: 0.00055, loss bc: 0.00013\n",
      "Epoch [760/10000], Loss: 0.00064, loss pde: 0.00000, loss ic: 0.00052, loss bc: 0.00011\n",
      "Epoch [780/10000], Loss: 0.00060, loss pde: 0.00000, loss ic: 0.00049, loss bc: 0.00011\n",
      "Epoch [800/10000], Loss: 0.00057, loss pde: 0.00000, loss ic: 0.00047, loss bc: 0.00010\n",
      "Epoch [820/10000], Loss: 0.00054, loss pde: 0.00000, loss ic: 0.00045, loss bc: 0.00009\n",
      "Epoch [840/10000], Loss: 0.00083, loss pde: 0.00028, loss ic: 0.00043, loss bc: 0.00012\n",
      "Epoch [860/10000], Loss: 0.00053, loss pde: 0.00003, loss ic: 0.00042, loss bc: 0.00008\n",
      "Epoch [880/10000], Loss: 0.00049, loss pde: 0.00001, loss ic: 0.00040, loss bc: 0.00008\n",
      "Epoch [900/10000], Loss: 0.00046, loss pde: 0.00000, loss ic: 0.00038, loss bc: 0.00008\n",
      "Epoch [920/10000], Loss: 0.00044, loss pde: 0.00000, loss ic: 0.00037, loss bc: 0.00007\n",
      "Epoch [940/10000], Loss: 0.00043, loss pde: 0.00000, loss ic: 0.00036, loss bc: 0.00007\n",
      "Epoch [960/10000], Loss: 0.00043, loss pde: 0.00001, loss ic: 0.00035, loss bc: 0.00007\n",
      "Epoch [980/10000], Loss: 0.00061, loss pde: 0.00018, loss ic: 0.00034, loss bc: 0.00009\n",
      "Epoch [1000/10000], Loss: 0.00042, loss pde: 0.00002, loss ic: 0.00033, loss bc: 0.00007\n",
      "Epoch [1020/10000], Loss: 0.00038, loss pde: 0.00000, loss ic: 0.00032, loss bc: 0.00006\n",
      "Epoch [1040/10000], Loss: 0.00037, loss pde: 0.00000, loss ic: 0.00031, loss bc: 0.00006\n",
      "Epoch [1060/10000], Loss: 0.00036, loss pde: 0.00000, loss ic: 0.00030, loss bc: 0.00006\n",
      "Epoch [1080/10000], Loss: 0.00035, loss pde: 0.00000, loss ic: 0.00029, loss bc: 0.00006\n",
      "Epoch [1100/10000], Loss: 0.00034, loss pde: 0.00000, loss ic: 0.00028, loss bc: 0.00006\n",
      "Epoch [1120/10000], Loss: 0.00037, loss pde: 0.00004, loss ic: 0.00028, loss bc: 0.00005\n",
      "Epoch [1140/10000], Loss: 0.00038, loss pde: 0.00005, loss ic: 0.00027, loss bc: 0.00005\n",
      "Epoch [1160/10000], Loss: 0.00032, loss pde: 0.00000, loss ic: 0.00026, loss bc: 0.00005\n",
      "Epoch [1180/10000], Loss: 0.00031, loss pde: 0.00000, loss ic: 0.00025, loss bc: 0.00005\n",
      "Epoch [1200/10000], Loss: 0.00030, loss pde: 0.00000, loss ic: 0.00025, loss bc: 0.00005\n",
      "Epoch [1220/10000], Loss: 0.00029, loss pde: 0.00000, loss ic: 0.00024, loss bc: 0.00005\n",
      "Epoch [1240/10000], Loss: 0.00028, loss pde: 0.00000, loss ic: 0.00023, loss bc: 0.00005\n",
      "Epoch [1260/10000], Loss: 0.00028, loss pde: 0.00000, loss ic: 0.00023, loss bc: 0.00005\n",
      "Epoch [1280/10000], Loss: 0.00033, loss pde: 0.00006, loss ic: 0.00022, loss bc: 0.00006\n",
      "Epoch [1300/10000], Loss: 0.00028, loss pde: 0.00001, loss ic: 0.00022, loss bc: 0.00005\n",
      "Epoch [1320/10000], Loss: 0.00026, loss pde: 0.00000, loss ic: 0.00021, loss bc: 0.00005\n",
      "Epoch [1340/10000], Loss: 0.00026, loss pde: 0.00000, loss ic: 0.00021, loss bc: 0.00005\n",
      "Epoch [1360/10000], Loss: 0.00025, loss pde: 0.00000, loss ic: 0.00020, loss bc: 0.00005\n",
      "Epoch [1380/10000], Loss: 0.00025, loss pde: 0.00000, loss ic: 0.00020, loss bc: 0.00005\n",
      "Epoch [1400/10000], Loss: 0.00024, loss pde: 0.00000, loss ic: 0.00020, loss bc: 0.00005\n",
      "Epoch [1420/10000], Loss: 0.00024, loss pde: 0.00000, loss ic: 0.00019, loss bc: 0.00004\n",
      "Epoch [1440/10000], Loss: 0.00023, loss pde: 0.00000, loss ic: 0.00019, loss bc: 0.00004\n",
      "Epoch [1460/10000], Loss: 0.00117, loss pde: 0.00086, loss ic: 0.00023, loss bc: 0.00007\n",
      "Epoch [1480/10000], Loss: 0.00030, loss pde: 0.00007, loss ic: 0.00019, loss bc: 0.00004\n",
      "Epoch [1500/10000], Loss: 0.00022, loss pde: 0.00000, loss ic: 0.00018, loss bc: 0.00004\n",
      "Epoch [1520/10000], Loss: 0.00022, loss pde: 0.00000, loss ic: 0.00018, loss bc: 0.00004\n",
      "Epoch [1540/10000], Loss: 0.00022, loss pde: 0.00000, loss ic: 0.00018, loss bc: 0.00004\n",
      "Epoch [1560/10000], Loss: 0.00021, loss pde: 0.00000, loss ic: 0.00017, loss bc: 0.00004\n",
      "Epoch [1580/10000], Loss: 0.00021, loss pde: 0.00000, loss ic: 0.00017, loss bc: 0.00004\n",
      "Epoch [1600/10000], Loss: 0.00021, loss pde: 0.00000, loss ic: 0.00017, loss bc: 0.00004\n",
      "Epoch [1620/10000], Loss: 0.00021, loss pde: 0.00000, loss ic: 0.00017, loss bc: 0.00004\n",
      "Epoch [1640/10000], Loss: 0.00020, loss pde: 0.00000, loss ic: 0.00016, loss bc: 0.00004\n",
      "Epoch [1660/10000], Loss: 0.00022, loss pde: 0.00002, loss ic: 0.00017, loss bc: 0.00004\n",
      "Epoch [1680/10000], Loss: 0.00026, loss pde: 0.00005, loss ic: 0.00016, loss bc: 0.00005\n",
      "Epoch [1700/10000], Loss: 0.00021, loss pde: 0.00001, loss ic: 0.00016, loss bc: 0.00004\n",
      "Epoch [1720/10000], Loss: 0.00020, loss pde: 0.00000, loss ic: 0.00016, loss bc: 0.00004\n",
      "Epoch [1740/10000], Loss: 0.00019, loss pde: 0.00000, loss ic: 0.00016, loss bc: 0.00004\n",
      "Epoch [1760/10000], Loss: 0.00019, loss pde: 0.00000, loss ic: 0.00016, loss bc: 0.00004\n",
      "Epoch [1780/10000], Loss: 0.00019, loss pde: 0.00000, loss ic: 0.00015, loss bc: 0.00004\n",
      "Epoch [1800/10000], Loss: 0.00019, loss pde: 0.00000, loss ic: 0.00015, loss bc: 0.00004\n",
      "Epoch [1820/10000], Loss: 0.00019, loss pde: 0.00000, loss ic: 0.00015, loss bc: 0.00004\n",
      "Epoch [1840/10000], Loss: 0.00018, loss pde: 0.00000, loss ic: 0.00015, loss bc: 0.00004\n",
      "Epoch [1860/10000], Loss: 0.00020, loss pde: 0.00001, loss ic: 0.00015, loss bc: 0.00004\n",
      "Epoch [1880/10000], Loss: 0.00039, loss pde: 0.00019, loss ic: 0.00016, loss bc: 0.00005\n",
      "Epoch [1900/10000], Loss: 0.00022, loss pde: 0.00004, loss ic: 0.00015, loss bc: 0.00004\n",
      "Epoch [1920/10000], Loss: 0.00018, loss pde: 0.00000, loss ic: 0.00015, loss bc: 0.00003\n",
      "Epoch [1940/10000], Loss: 0.00018, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [1960/10000], Loss: 0.00018, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [1980/10000], Loss: 0.00017, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2000/10000], Loss: 0.00017, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2020/10000], Loss: 0.00017, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2040/10000], Loss: 0.00017, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2060/10000], Loss: 0.00017, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2080/10000], Loss: 0.00019, loss pde: 0.00002, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2100/10000], Loss: 0.00022, loss pde: 0.00005, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2120/10000], Loss: 0.00017, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2140/10000], Loss: 0.00017, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2160/10000], Loss: 0.00016, loss pde: 0.00000, loss ic: 0.00014, loss bc: 0.00003\n",
      "Epoch [2180/10000], Loss: 0.00016, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00003\n",
      "Epoch [2200/10000], Loss: 0.00016, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00003\n",
      "Epoch [2220/10000], Loss: 0.00016, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00003\n",
      "Epoch [2240/10000], Loss: 0.00016, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00003\n",
      "Epoch [2260/10000], Loss: 0.00110, loss pde: 0.00087, loss ic: 0.00017, loss bc: 0.00006\n",
      "Epoch [2280/10000], Loss: 0.00025, loss pde: 0.00008, loss ic: 0.00013, loss bc: 0.00003\n",
      "Epoch [2300/10000], Loss: 0.00017, loss pde: 0.00002, loss ic: 0.00013, loss bc: 0.00003\n",
      "Epoch [2320/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2340/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2360/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2380/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2400/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2420/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2440/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2460/10000], Loss: 0.00071, loss pde: 0.00052, loss ic: 0.00015, loss bc: 0.00004\n",
      "Epoch [2480/10000], Loss: 0.00024, loss pde: 0.00008, loss ic: 0.00013, loss bc: 0.00003\n",
      "Epoch [2500/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2520/10000], Loss: 0.00015, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2540/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2560/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2580/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2600/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2620/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2640/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2660/10000], Loss: 0.00030, loss pde: 0.00014, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2680/10000], Loss: 0.00022, loss pde: 0.00008, loss ic: 0.00013, loss bc: 0.00002\n",
      "Epoch [2700/10000], Loss: 0.00015, loss pde: 0.00001, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2720/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2740/10000], Loss: 0.00014, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2760/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2780/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2800/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2820/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2840/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00012, loss bc: 0.00002\n",
      "Epoch [2860/10000], Loss: 0.00013, loss pde: 0.00001, loss ic: 0.00012, loss bc: 0.00001\n",
      "Epoch [2880/10000], Loss: 0.00064, loss pde: 0.00046, loss ic: 0.00014, loss bc: 0.00004\n",
      "Epoch [2900/10000], Loss: 0.00014, loss pde: 0.00001, loss ic: 0.00011, loss bc: 0.00002\n",
      "Epoch [2920/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [2940/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [2960/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [2980/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3000/10000], Loss: 0.00013, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3020/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3040/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3060/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3080/10000], Loss: 0.00160, loss pde: 0.00135, loss ic: 0.00016, loss bc: 0.00009\n",
      "Epoch [3100/10000], Loss: 0.00013, loss pde: 0.00001, loss ic: 0.00011, loss bc: 0.00002\n",
      "Epoch [3120/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3140/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3160/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3180/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3200/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3220/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3240/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3260/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3280/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3300/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3320/10000], Loss: 0.00086, loss pde: 0.00067, loss ic: 0.00013, loss bc: 0.00006\n",
      "Epoch [3340/10000], Loss: 0.00023, loss pde: 0.00010, loss ic: 0.00011, loss bc: 0.00002\n",
      "Epoch [3360/10000], Loss: 0.00013, loss pde: 0.00001, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3380/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3400/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3420/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3440/10000], Loss: 0.00012, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3460/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3480/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3500/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3520/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3540/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3560/10000], Loss: 0.00116, loss pde: 0.00097, loss ic: 0.00013, loss bc: 0.00007\n",
      "Epoch [3580/10000], Loss: 0.00023, loss pde: 0.00011, loss ic: 0.00011, loss bc: 0.00001\n",
      "Epoch [3600/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3620/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3640/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3660/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3680/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3700/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3720/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3740/10000], Loss: 0.00020, loss pde: 0.00008, loss ic: 0.00010, loss bc: 0.00002\n",
      "Epoch [3760/10000], Loss: 0.00025, loss pde: 0.00012, loss ic: 0.00010, loss bc: 0.00002\n",
      "Epoch [3780/10000], Loss: 0.00014, loss pde: 0.00003, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3800/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3820/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3840/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3860/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3880/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3900/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3920/10000], Loss: 0.00012, loss pde: 0.00001, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3940/10000], Loss: 0.00038, loss pde: 0.00024, loss ic: 0.00012, loss bc: 0.00003\n",
      "Epoch [3960/10000], Loss: 0.00012, loss pde: 0.00001, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [3980/10000], Loss: 0.00011, loss pde: 0.00001, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4000/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4020/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4040/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4060/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4080/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4100/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4120/10000], Loss: 0.00021, loss pde: 0.00010, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4140/10000], Loss: 0.00017, loss pde: 0.00006, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4160/10000], Loss: 0.00012, loss pde: 0.00001, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4180/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4200/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4220/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4240/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4260/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4280/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4300/10000], Loss: 0.00137, loss pde: 0.00116, loss ic: 0.00014, loss bc: 0.00006\n",
      "Epoch [4320/10000], Loss: 0.00012, loss pde: 0.00001, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4340/10000], Loss: 0.00012, loss pde: 0.00001, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4360/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4380/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4400/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4420/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4440/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4460/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4480/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4500/10000], Loss: 0.00011, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4520/10000], Loss: 0.00022, loss pde: 0.00011, loss ic: 0.00010, loss bc: 0.00001\n",
      "Epoch [4540/10000], Loss: 0.00015, loss pde: 0.00004, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4560/10000], Loss: 0.00011, loss pde: 0.00001, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4580/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4600/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4620/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "Epoch [4640/10000], Loss: 0.00010, loss pde: 0.00000, loss ic: 0.00009, loss bc: 0.00001\n",
      "stop optimization\n"
     ]
    }
   ],
   "source": [
    "# параметры для расчета loss_pde\n",
    "#ka, kb = 0.02, 0.1\n",
    "ka, kb = 0.00, 0.3\n",
    "# весовые коэффициенты для общего loss\n",
    "alpha_pde, alpha_ic, alpha_bc = 1.,1.,1.\n",
    "# обучение\n",
    "MAX = 10000\n",
    "for epoch in range(MAX):\n",
    "    # ---------------------------------- ПРЕДИКТИВНЫЕ ДАННЫЕ ПО ВСЕЙ ОБЛАСТИ ОПРЕДЕЛЕНИЯ ----------------\n",
    "    u_pred = u_net(x_batch,t_batch)\n",
    "    # ---------------------------------- LOSS PDE ----------------\n",
    "    u_pred_pde = u_pred[0:72]\n",
    "    # du/dx, du/dt\n",
    "    [du_dx, du_dt] = torch.autograd.grad(u_pred_pde, inputs=[x_batch_pde, t_batch_pde], grad_outputs=torch.ones_like(x_batch_pde), create_graph=True, retain_graph=True)\n",
    "    # d2u/dx2\n",
    "    [d2u_dx2, du_dxdt] = torch.autograd.grad(du_dx, inputs=[x_batch_pde, t_batch_pde], grad_outputs=torch.ones_like(x_batch_pde), create_graph=True, retain_graph=True)\n",
    "    # loss pde\n",
    "    loss_pde = torch.mean((du_dt[0] + ka*2*torch.pi*torch.sin(2*torch.pi*x_batch_pde)*du_dx[0] - (ka*torch.cos(2*torch.pi*x_batch_pde) +kb)*d2u_dx2[0])**2)\n",
    "    # ---------------------------------- LOSS IC -----------------\n",
    "    u_pred_ic = u_pred[72:82]\n",
    "    loss_ic = torch.mean( (u_pred_ic - torch.sin(torch.pi*x_batch_ic))**2 )\n",
    "    # ---------------------------------- LOSS BC -----------------\n",
    "    u_pred_bc = u_pred[82:100]\n",
    "    loss_bc = torch.mean( (u_pred_bc)**2 )\n",
    "    # ---------------------------------- LOSS COMMON -------------\n",
    "    loss = alpha_pde * loss_pde + alpha_ic * loss_ic + alpha_bc * loss_bc\n",
    "    # стираем предыдущий граф градиента\n",
    "    optimizer.zero_grad()\n",
    "    # обратное распространение ошибки\n",
    "    loss.backward()\n",
    "    # записываем новые коэфициента\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch [{epoch}/{MAX}], Loss: {loss.item():.5f}, loss pde: {loss_pde.item():.5f}, loss ic: {loss_ic.item():.5f}, loss bc: {loss_bc.item():.5f}')\n",
    "    if loss < 0.0001:\n",
    "        print('stop optimization')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17c82c-24db-4aea-8edf-3fd3607d429b",
   "metadata": {},
   "source": [
    "# 1.7 Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfe54f2-6f69-4757-9f62-41a215f14890",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_res = u_net(x_batch,t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a08f8483-b43c-40f9-bd4f-156ca936ea15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0189],\n",
       "        [0.3233],\n",
       "        [0.6374],\n",
       "        [0.8760],\n",
       "        [0.9891],\n",
       "        [0.9801],\n",
       "        [0.8650],\n",
       "        [0.6468],\n",
       "        [0.3385],\n",
       "        [0.0024]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial condition\n",
    "u_res[72:82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce63d9d-a0e0-4b1d-915d-0100a98dc40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4037e-04],\n",
       "        [-7.5166e-03],\n",
       "        [-4.9389e-03],\n",
       "        [ 1.0121e-03],\n",
       "        [ 3.6427e-03],\n",
       "        [ 1.1681e-03],\n",
       "        [-2.6031e-03],\n",
       "        [-2.5817e-03],\n",
       "        [ 3.1445e-03],\n",
       "        [-3.6415e-03],\n",
       "        [ 2.2906e-04],\n",
       "        [ 2.7710e-03],\n",
       "        [ 1.1981e-03],\n",
       "        [-1.1494e-03],\n",
       "        [-2.0152e-03],\n",
       "        [-1.3506e-03],\n",
       "        [ 9.4175e-05],\n",
       "        [ 1.6629e-03]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boundary condition\n",
    "u_res[82:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "794ebe82-045e-4152-9932-c112837d5466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3199, 0.3137, 0.2952, 0.2538, 0.1923, 0.1291, 0.0819, 0.0556, 0.0449],\n",
       "        [0.6354, 0.6036, 0.5247, 0.4012, 0.2695, 0.1676, 0.1055, 0.0733, 0.0580],\n",
       "        [0.8490, 0.7609, 0.6013, 0.4112, 0.2540, 0.1540, 0.0992, 0.0710, 0.0563],\n",
       "        [0.9168, 0.7621, 0.5450, 0.3416, 0.2029, 0.1241, 0.0824, 0.0603, 0.0479],\n",
       "        [0.8596, 0.6579, 0.4315, 0.2561, 0.1502, 0.0932, 0.0633, 0.0470, 0.0376],\n",
       "        [0.7177, 0.5113, 0.3154, 0.1811, 0.1052, 0.0656, 0.0450, 0.0338, 0.0273],\n",
       "        [0.5176, 0.3539, 0.2113, 0.1183, 0.0673, 0.0414, 0.0283, 0.0216, 0.0179],\n",
       "        [0.2697, 0.1854, 0.1110, 0.0610, 0.0331, 0.0193, 0.0131, 0.0105, 0.0094]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDE\n",
    "res = u_res[0:72].view(8,9)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f193e717-9927-4b99-bd19-fbb55ec20589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = torch.zeros(10,10)\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9d7dd7-fe53-4f96-a2f9-99ea3ba4faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[:,0] = 1\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ebbfe8a-d65f-4bb3-b4f4-11919517030d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[10, 1]}, size=[10]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mU\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m u_res[\u001b[38;5;241m72\u001b[39m:\u001b[38;5;241m82\u001b[39m] \u001b[38;5;66;03m#.detach().numpy()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m U\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[10, 1]}, size=[10]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "U[:,0] = u_res[72:82] #.detach().numpy()\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28406d8b-8d2c-4f44-a15b-8dca040f8c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1713f2-bec5-414e-868b-67fe1c4867c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0189],\n",
       "        [0.3233],\n",
       "        [0.6374],\n",
       "        [0.8760],\n",
       "        [0.9891],\n",
       "        [0.9801],\n",
       "        [0.8650],\n",
       "        [0.6468],\n",
       "        [0.3385],\n",
       "        [0.0024]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_res[72:82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c4584851-700d-4ca4-bb20-5cc4e3c31444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb508326-5b31-4d63-91a6-ad6f14884087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
