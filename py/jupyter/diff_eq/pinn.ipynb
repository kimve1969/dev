{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9a4b21-d8d9-46af-bf6f-77c1184a046a",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "Теоретическая основа: универсальная теорема аппроксимации - теорема Хорника-Цирельсона-Уайта: \\\n",
    "Однослойная нейроная сеть с достаточно большим числом нейронов и нелинейной гладкой функцией \\\n",
    "(например, sigmoid или tanh) может приблизить любую непрерывную функцию на компактном множестве \\\n",
    "с любой заданной точностью. \n",
    "\n",
    "$$\n",
    "sup_{x \\in K}| f(x) - N(x) | < \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0bfc7-18fc-4b56-8e59-37cba3dcd992",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Уравнение теплопроводности\n",
    "\n",
    "$$ \\frac{\\partial u(x,t)}{\\partial t}=\\frac{\\partial}{\\partial x}(k(x)*\\frac{\\partial u(x,t)}{\\partial x})+f(x,t)) $$\n",
    "$$ x \\in (0,L), \\, t \\in (0,T] $$ \\\n",
    "$ u(x,t) $ - температура  \\\n",
    "$ k(x) $ - коэф.теплопроводности \\\n",
    "$ f(x,t) $ - источники \\\\ стоки\n",
    "### Начальные условия:\n",
    "$$ u(x,0)=g_{IC}(x) $$\n",
    "$$ x \\in [0,L], \\, t = 0 $$\n",
    "### Граничные условия (например, первого рода):\n",
    "$$ u(0,t)=g_{LB}(t), u(L,t)=g_{RB}(t) $$\n",
    "$$ x \\in \\{0,L\\}, \\, t \\in (0,T] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ae47f-ef7b-4e7b-9b07-5a7d6f99e630",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Аппроксимация дифференциального уравнения нейроной сетью\n",
    "$ N_u(x,t) $ - нейросеть аппроксимирует u(x,t),тогда аппроксимированное ур-ие теплопроводности примет вид:\n",
    "$$\n",
    "\\frac{\\partial N_u(x,t)}{\\partial t}=\\frac{\\partial}{\\partial x}(k(x) \\frac{\\partial N_u(x,t)}{\\partial x})+f(x,t))\n",
    "$$\n",
    "\n",
    "или раскроем производную произведения и перенесем все в левую сторону\n",
    "$$\n",
    "\\frac{\\partial N_u(x,t)}{\\partial t} - \\frac{\\partial k(x)}{\\partial x} \\frac{\\partial N_u(x,t)}{\\partial x} - k(x) \\frac{\\partial^2 N_u(x,t)}{\\partial x^2} -f(x,t)) = 0\n",
    "$$\n",
    "\n",
    "Производная нейросети $ N_u(x,t) $, состоящая, например, из входа -> два внутренних слоя -> выход - \n",
    "это гладкая функция как композиция слоев нейросети \\\n",
    "$$ N_u(x,t) = f_3(f_2(f_1(x,t,\\theta), \\theta), \\theta), $$ \\\n",
    "$ f_1, f_2, f_3 $ - слои нейросети (линейные преобразования + функции активации) \\\n",
    "$ \\theta_i $ - обучаемые параметры (веса и смещения) \\\n",
    "производная $  N_u(x,t) $ вычисляется по цепному правилу:\n",
    "$$\n",
    "\\frac{\\partial N_u}{\\partial x} = \\frac{\\partial f_3}{\\partial f_2} * \\frac{\\partial f_2}{\\partial f_1} * \\frac{\\partial f_1}{\\partial x}\n",
    "$$\n",
    "где каждый член это якобиан (матрица частных производных)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb32d2a-6e34-4abd-8f53-2740e7ab5440",
   "metadata": {},
   "source": [
    "# Функция потерь (loss function)\n",
    "### 1. Partial Differential Equation loss: ошибка аппроксимации дифф.ур-ия нейронной сетью\n",
    "$$\n",
    "L_{PDE} = \\frac{1}{M} \\sum_{j=1}^M || \\frac{\\partial N_u(x,t)}{\\partial t} - \\frac{\\partial k(x)}{\\partial x} \\frac{\\partial N_u(x,t)}{\\partial x} - k(x) \\frac{\\partial^2 N_u(x,t)}{\\partial x^2} -f(x,t) ||^2\n",
    "$$\n",
    "$$ x \\in (0,L), t \\in (0,T] $$\n",
    "### 2. Initial condition loss: ошибка нейронной сети по начальным условиям\n",
    "$$ L_{IC} = \\frac{1}{K} \\sum_{j=1}^K || N_u(x,0) - g_{IC}(x) ||^2 $$\n",
    "$$ x \\in [0;L], t = 0 $$\n",
    "### 3. Boundary condition loss: ошибка нейронной сети по граничным условиям\n",
    "$$ L_{BC} = \\frac{1}{P} (\\sum_{j=1}^P || N_u(0,t) - g_{LB}(t) ||^2 + \\sum_{j=1}^P || N_u(L,t) - g_{RB}(t) ||^2) $$\n",
    "$$ x \\in \\{0,L\\}, t \\in (0,T] $$\n",
    "### 4. Data loss: ошибка предсказанных и экспериментальных данных\n",
    "$$ L_{data} = \\frac{1}{N} \\sum_{j=1}^N || N_u(x,t) - u_{data}(x,t) ||^2 $$\n",
    "$$ x \\in (0,L), t \\in (0,T] $$\n",
    "### Итоговый лосс:\n",
    "$$ L = \\lambda_{PDE} L_{PDE} + \\lambda_{IC} L_{IC} + \\lambda_{BC} L_{BC} + \\lambda_{data} L_{data} $$\n",
    "пояснение по выбору M,K,P,N пусть \n",
    "K - число пропорциональное кол-ву шагов дискретизации по времени, \n",
    "P - число пропорциональное кол-ву шагов дискретизации по пространству (в данном случае одномерному)\n",
    "тогда \n",
    "M - будет числом пропорциональным K*P кол-ву шагов дискретизации внутренней области определения\n",
    "N - кол-во возможных экспериментальных данных, которые попадают во внутреннюю область \\\n",
    "Таким образом:\n",
    "1. Решаем ПРЯМУЮ ЗАДАЧУ - на выходе получаем PINN физической модели (аппроксимацию дифференциального уравнения в частных производных с учетом нач. и гр. условий)\n",
    "$$ L_{phys} = \\lambda_{PDE} L_{PDE} + \\lambda_{IC} L_{IC} + \\lambda_{BC} L_{BC} $$\n",
    "3. Решаем ОБРАТНУЮ ЗАДАЧУ - дополнительно к аппроксимации физической модели $ N_u(x,t) $ добавляем в уравнение аппроксимацию, например, коэффициента теплопроводности $ N_k(x) $ и решаем задачу оптимизации двух PINN сетей  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef0923-b2bc-46b2-9468-0dd0a2b254e5",
   "metadata": {},
   "source": [
    "Пояснение по виду лоссов: $ loss = \\frac{1}{N} \\sum_{j=1}^N ||x||^2  $, где ||*|| - Евклидова норма. \\\n",
    "Если x - вектор независимых параметров, т.е. $ x \\in \\{x_1, x_2, ..., x_n\\} $\n",
    "то $ ||x|| = \\sqrt {x_1^2 + x_2^2 + ... x_n^2} $ (длина n-мерного вектора)  или $ ||x||^2 = \\sum_{i=1}^n x_i^2 $, тогда:\n",
    "$ loss =  \\frac{1}{N} \\sum_{j=1}^N (\\sum_{i=1}^n x_i^2) $, где первая сумма (справа) - длина n-мерного вектора в квадрате,\n",
    "а вторая сумма (слева) - среднее значение N длин в квадрате n-мерных векторов \\\n",
    "Если x - это скаляр, тогда: $ loss = \\frac{1}{N} \\sum_{j=1}^N (x^2) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439210de-c544-419e-80c6-99204b2d919e",
   "metadata": {},
   "source": [
    "# ПРИМЕР\n",
    "## 1. ПРЯМАЯ ЗАДАЧА\n",
    "### Пусть исходные данные следующие:\n",
    "$$ x \\in [0,1], \\, t \\in [0,1] $$\n",
    "$$ u(0,t) = u(L,t)=0 $$\n",
    "$$ u(x,0) = sin(\\pi * x) $$\n",
    "$$ k(x) = k_a \\, cos(2 \\pi x) + k_b $$ \n",
    "$$ \\frac{\\partial k(x)}{\\partial x} = - k_a 2 \\pi \\, sin(2 \\pi x) $$\n",
    "$$ f(x,t) = 0 $$\n",
    "Значения $ k_a $ и $ k_b $ такие, что $ k(x) $ положителен на всем интервале x тогда:\n",
    "### Partial Differential Equation loss:\n",
    "$$\n",
    "L_{PDE} = \\frac{1}{M} \\sum_{j=1}^M || \\frac{\\partial N_u(x,t)}{\\partial t} + k_a 2 \\pi \\, sin(2 \\pi x)*\\frac{\\partial N_u(x,t)}{\\partial x} - (k_a \\, cos(2 \\pi x) + k_b) *\\frac{\\partial^2 N_u(x,t)}{\\partial x^2} ||^2\n",
    "$$ \n",
    "$$ x \\in (0;L), t \\in (0;T] $$\n",
    "### Initial condition loss:\n",
    "$$ L_{IC} = \\frac{1}{K} \\sum_{j=1}^K || N_u(x,0) - sin(\\pi * x) ||^2 $$\n",
    "$$ x \\in [0;L], t = 0 $$\n",
    "### Boundary condition loss:\n",
    "$$ L_{BC} = \\frac{1}{P} (\\sum_{j=1}^P || N_u(0,t) ||^2 + \\sum_{j=1}^P || N_u(L,t) ||^2) $$\n",
    "$$ x=\\{0,L\\}, t \\in (0;T] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530f509-3f15-4a3d-9bae-797cfe25ea1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1.1 Создаем PDE нейроную сеть\n",
    "### 2 (вход) -> 5 (1-ый внутр.слой) -> 5 (2-ой внутр.слой) -> 1 (выход)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1400555e-e269-4529-ba74-bd61586d0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем нейросеть 2->5->5->1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Nu( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2,5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5,5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.fc( torch.cat([x, t], dim=1) ) # self.layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "c315ad1d-bb6e-4542-b529-419f5bd05e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nu(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net = Nu()\n",
    "u_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357a30b-c37f-46d1-9940-6f7202b998b7",
   "metadata": {},
   "source": [
    "# 1.2 Проверка работы нейросети (вручную проходим от входа на выход по всем слоям)\n",
    "### 1.2.1 2 нейрона (вход) -> 5 нейронов (1-ый внутр.слой)\n",
    "$$\n",
    "h_i = Sigmoid(\\sum_{j=1}^2 w_{ij}x_j + b_i), i = 1,2,3,4,5\n",
    "$$\n",
    "\n",
    "или в матричном кратком виде: $ h = Sigmoid(W*x+b) $, или в матричном полном виде:\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "h_4 \\\\\n",
    "h_5\n",
    "\\end{vmatrix} \\quad = \\quad\n",
    "Sigmoid\n",
    "\\begin{pmatrix}\n",
    "\\begin{vmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32} \\\\\n",
    "w_{41} & w_{42} \\\\\n",
    "w_{51} & w_{52}\n",
    "\\end{vmatrix} *\n",
    "\\begin{vmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{vmatrix} \\quad + \\quad\n",
    "\\begin{vmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4 \\\\\n",
    "b_5\n",
    "\\end{vmatrix}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a124f725-7fef-439d-b570-8a853a501c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.2000]])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = torch.tensor([0.1, 0.2]).view(-1,1)\n",
    "xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "59e928a0-341e-477c-b160-0780c511a2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2342, -0.3388],\n",
      "        [ 0.6455, -0.4229],\n",
      "        [ 0.1555, -0.3089],\n",
      "        [ 0.0162,  0.1574],\n",
      "        [-0.1670,  0.0816]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1585,  0.5974, -0.4809, -0.5537,  0.2920], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2-> 5\n",
    "print(u_net.fc[0].weight)\n",
    "print(u_net.fc[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "9f9d897e-0155-4e36-942d-b3cfe6c99c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4495],\n",
       "        [0.6405],\n",
       "        [0.3712],\n",
       "        [0.3727],\n",
       "        [0.5724]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# h = Sigmoid( W(1)x+b(1) )\n",
    "h = torch.sigmoid( u_net.fc[0].weight @ xt + u_net.fc[0].bias.view(-1,1) )\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016ae05-5944-438b-805a-86e530446006",
   "metadata": {},
   "source": [
    "### 1.2.2 5 нейронов (1-ый внутр.слой) -> 5 нейронов (2-ый внутр.слой)\n",
    "$$\n",
    "g_i = Sigmoid(\\sum_{j=1}^5 w_{ij}h_j + b_i), i = 1,2,3,4,5\n",
    "$$\n",
    "\n",
    "или в матричном кратком виде: $ g = Sigmoid(W*h+b) $, или в матричном полном виде:\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "g_1 \\\\\n",
    "g_2 \\\\\n",
    "g_3 \\\\\n",
    "g_4 \\\\\n",
    "g_5\n",
    "\\end{vmatrix} \\quad = \\quad\n",
    "Sigmoid\n",
    "\\begin{pmatrix}\n",
    "\\begin{vmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} & w_{15} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} & w_{25} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34} & w_{35} \\\\\n",
    "w_{41} & w_{42} & w_{43} & w_{44} & w_{45} \\\\\n",
    "w_{51} & w_{52} & w_{53} & w_{54} & w_{55} \n",
    "\\end{vmatrix} *\n",
    "\\begin{vmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "h_4 \\\\\n",
    "h_5\n",
    "\\end{vmatrix} \\quad + \\quad\n",
    "\\begin{vmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4 \\\\\n",
    "b_5\n",
    "\\end{vmatrix}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "531df2ed-c8ea-4cf6-9e2c-ba50ed65f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1485, -0.0145, -0.3144, -0.0176, -0.3779],\n",
      "        [ 0.2724,  0.1902,  0.4231,  0.3120, -0.0301],\n",
      "        [-0.0023,  0.4074,  0.1434,  0.1106, -0.2750],\n",
      "        [ 0.1982,  0.2734, -0.3780,  0.0938,  0.3383],\n",
      "        [-0.1747,  0.4389,  0.2323, -0.4004,  0.3492]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3671,  0.1401, -0.4464, -0.2205,  0.0322], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 5-> 5\n",
    "print(u_net.fc[2].weight)\n",
    "print(u_net.fc[2].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "064fbc0d-b338-426d-a6b8-c35ea635bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4879],\n",
       "        [0.6549],\n",
       "        [0.4379],\n",
       "        [0.5330],\n",
       "        [0.5918]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g = Sigmoid( W(2)h+b(2) )\n",
    "g = torch.sigmoid( u_net.fc[2].weight @ h + u_net.fc[2].bias.view(-1,1) )\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a980c-b802-43f4-9565-60cd23f43467",
   "metadata": {},
   "source": [
    "### 1.2.3 5 нейронов (2-ый внутр.слой) -> 1 нейрон (выход)\n",
    "$$\n",
    "y_i = \\sum_{j=1}^5 w_{ij}g_j + b_i, i = 1\n",
    "$$\n",
    "\n",
    "или в матричном кратком виде: $ y = W*g+b $, или в матричном полном виде:\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "y_1\n",
    "\\end{vmatrix} \\quad = \\quad\n",
    "\\begin{vmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} & w_{15} \\\\\n",
    "\\end{vmatrix} *\n",
    "\\begin{vmatrix}\n",
    "g_1 \\\\\n",
    "g_2 \\\\\n",
    "g_3 \\\\\n",
    "g_4 \\\\\n",
    "g_5\n",
    "\\end{vmatrix} \\quad + \\quad\n",
    "\\begin{vmatrix}\n",
    "b_1\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Обратите внимание, что функции активации нет, но могла бы быть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "8f2edd52-981f-454a-9946-a696514d0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2195, 0.1177, 0.0355, 0.0253, 0.3442]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3699], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 5 -> 1\n",
    "print(u_net.fc[4].weight)\n",
    "print(u_net.fc[4].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "a9f51531-2720-4965-a351-2960919620f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0470]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = W(3)g+b(3)\n",
    "y = u_net.fc[4].weight @ g + u_net.fc[4].bias.view(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94d001-7036-42db-8e5a-950c88e9539c",
   "metadata": {},
   "source": [
    "# 1.3 Проверка ручных вычислений с автоматическим (передачей параметров в нейроную сеть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e9236b81-59f9-4edb-8fdb-717b29ea9bb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.2000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0470]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# придется входные данные представить немного в ином виде, т.к. в модели нейронки Nu параметры передаются\n",
    "# в пакетном batch режиме\n",
    "x_batch = xt[0].view(-1,1)\n",
    "t_batch = xt[1].view(-1,1)\n",
    "print(torch.cat([x_batch,t_batch], dim=1))\n",
    "# предсказания, на данном этапе нейронка инициализируется каким-то начальными значениями, \n",
    "# соответсвенно предсказание носит некий случайный характер в начале\n",
    "u_pred = u_net(x_batch,t_batch)\n",
    "u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "a6996b80-5642-444f-9231-93856d75fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if y == u_pred:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2034bcd-5ead-48ce-b6a1-38ec39650caa",
   "metadata": {},
   "source": [
    "# 1.4 Вычисляем градиенты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d035c-3120-4ce5-aa75-cddc47fb8ece",
   "metadata": {},
   "source": [
    "### $ x \\in (0,1) \\, , t \\in (0,1] $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a077f-4f7e-4e66-a1f5-e06f7e83575d",
   "metadata": {},
   "source": [
    "# 1.5 Вычисляем лоссы\n",
    "## 1.5.1 Лосс PDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b27f6-adef-4740-bd40-3ea9b3c141dc",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{PDE} = \\frac{1}{M} \\sum_{j=1}^M || \\frac{\\partial N_u(x,t)}{\\partial t} + k_a 2 \\pi \\, sin(2 \\pi x)*\\frac{\\partial N_u(x,t)}{\\partial x} - (k_a \\, cos(2 \\pi x) + k_b) *\\frac{\\partial^2 N_u(x,t)}{\\partial x^2} ||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa280801-f45a-427e-a732-78ec3dde82a8",
   "metadata": {},
   "source": [
    "$$ k_a = 0.02 $$\n",
    "$$ k_b = 0.1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ca0245c2-f9c1-4c7e-b086-9967c17ad5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka, kb = 0.02, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "d1d089fe-bf2e-46be-85ed-5f38aa8ad8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net = Nu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1785e5-d230-461b-b2c6-e472adbd2b74",
   "metadata": {},
   "source": [
    "# С этого места глобальный цикл по всем 3-м лоссам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "f12af9f8-23b7-49bd-9836-92aea5c6fc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.linspace(0,1,12)[1:11]\n",
    "x_batch = torch.empty(0)\n",
    "\n",
    "for x in temp:\n",
    "    x_batch = torch.cat( [x_batch, x * torch.ones( temp.shape[0] ) ])\n",
    "x_batch = x_batch.view(-1,1)\n",
    "x_batch.requires_grad = True\n",
    "x_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "142f040e-b2e3-45ff-ab63-3833c30905d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_batch = torch.empty(0)\n",
    "\n",
    "for i in temp:\n",
    "    t_batch = torch.cat( [ t_batch, torch.linspace(0,1,11)[1:11] ] )\n",
    "t_batch = t_batch.view(-1,1)\n",
    "t_batch.requires_grad = True\n",
    "t_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "9b26525b-e42e-413d-a973-0968dec87dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt_batch = torch.cat( [ x_batch, t_batch], dim=1 )\n",
    "xt_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "6a49192b-da82-4586-9c28-8633655bbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_pde = torch.optim.Adam( u_net.parameters(), lr=1e-5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "2360e264-e07d-42ee-9ee2-4eacd355045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], Loss: 0.04810\n",
      "Epoch [20/10000], Loss: 0.04803\n",
      "Epoch [40/10000], Loss: 0.04795\n",
      "Epoch [60/10000], Loss: 0.04788\n",
      "Epoch [80/10000], Loss: 0.04781\n",
      "Epoch [100/10000], Loss: 0.04773\n",
      "Epoch [120/10000], Loss: 0.04766\n",
      "Epoch [140/10000], Loss: 0.04758\n",
      "Epoch [160/10000], Loss: 0.04751\n",
      "Epoch [180/10000], Loss: 0.04743\n",
      "Epoch [200/10000], Loss: 0.04736\n",
      "Epoch [220/10000], Loss: 0.04729\n",
      "Epoch [240/10000], Loss: 0.04721\n",
      "Epoch [260/10000], Loss: 0.04714\n",
      "Epoch [280/10000], Loss: 0.04706\n",
      "Epoch [300/10000], Loss: 0.04699\n",
      "Epoch [320/10000], Loss: 0.04692\n",
      "Epoch [340/10000], Loss: 0.04684\n",
      "Epoch [360/10000], Loss: 0.04677\n",
      "Epoch [380/10000], Loss: 0.04670\n",
      "Epoch [400/10000], Loss: 0.04662\n",
      "Epoch [420/10000], Loss: 0.04655\n",
      "Epoch [440/10000], Loss: 0.04648\n",
      "Epoch [460/10000], Loss: 0.04640\n",
      "Epoch [480/10000], Loss: 0.04633\n",
      "Epoch [500/10000], Loss: 0.04626\n",
      "Epoch [520/10000], Loss: 0.04618\n",
      "Epoch [540/10000], Loss: 0.04611\n",
      "Epoch [560/10000], Loss: 0.04604\n",
      "Epoch [580/10000], Loss: 0.04597\n",
      "Epoch [600/10000], Loss: 0.04589\n",
      "Epoch [620/10000], Loss: 0.04582\n",
      "Epoch [640/10000], Loss: 0.04575\n",
      "Epoch [660/10000], Loss: 0.04568\n",
      "Epoch [680/10000], Loss: 0.04561\n",
      "Epoch [700/10000], Loss: 0.04553\n",
      "Epoch [720/10000], Loss: 0.04546\n",
      "Epoch [740/10000], Loss: 0.04539\n",
      "Epoch [760/10000], Loss: 0.04532\n",
      "Epoch [780/10000], Loss: 0.04525\n",
      "Epoch [800/10000], Loss: 0.04517\n",
      "Epoch [820/10000], Loss: 0.04510\n",
      "Epoch [840/10000], Loss: 0.04503\n",
      "Epoch [860/10000], Loss: 0.04496\n",
      "Epoch [880/10000], Loss: 0.04489\n",
      "Epoch [900/10000], Loss: 0.04482\n",
      "Epoch [920/10000], Loss: 0.04474\n",
      "Epoch [940/10000], Loss: 0.04467\n",
      "Epoch [960/10000], Loss: 0.04460\n",
      "Epoch [980/10000], Loss: 0.04453\n",
      "Epoch [1000/10000], Loss: 0.04446\n",
      "Epoch [1020/10000], Loss: 0.04439\n",
      "Epoch [1040/10000], Loss: 0.04432\n",
      "Epoch [1060/10000], Loss: 0.04425\n",
      "Epoch [1080/10000], Loss: 0.04418\n",
      "Epoch [1100/10000], Loss: 0.04411\n",
      "Epoch [1120/10000], Loss: 0.04404\n",
      "Epoch [1140/10000], Loss: 0.04397\n",
      "Epoch [1160/10000], Loss: 0.04390\n",
      "Epoch [1180/10000], Loss: 0.04383\n",
      "Epoch [1200/10000], Loss: 0.04376\n",
      "Epoch [1220/10000], Loss: 0.04369\n",
      "Epoch [1240/10000], Loss: 0.04362\n",
      "Epoch [1260/10000], Loss: 0.04355\n",
      "Epoch [1280/10000], Loss: 0.04348\n",
      "Epoch [1300/10000], Loss: 0.04341\n",
      "Epoch [1320/10000], Loss: 0.04334\n",
      "Epoch [1340/10000], Loss: 0.04327\n",
      "Epoch [1360/10000], Loss: 0.04320\n",
      "Epoch [1380/10000], Loss: 0.04313\n",
      "Epoch [1400/10000], Loss: 0.04306\n",
      "Epoch [1420/10000], Loss: 0.04299\n",
      "Epoch [1440/10000], Loss: 0.04292\n",
      "Epoch [1460/10000], Loss: 0.04285\n",
      "Epoch [1480/10000], Loss: 0.04278\n",
      "Epoch [1500/10000], Loss: 0.04271\n",
      "Epoch [1520/10000], Loss: 0.04264\n",
      "Epoch [1540/10000], Loss: 0.04257\n",
      "Epoch [1560/10000], Loss: 0.04251\n",
      "Epoch [1580/10000], Loss: 0.04244\n",
      "Epoch [1600/10000], Loss: 0.04237\n",
      "Epoch [1620/10000], Loss: 0.04230\n",
      "Epoch [1640/10000], Loss: 0.04223\n",
      "Epoch [1660/10000], Loss: 0.04216\n",
      "Epoch [1680/10000], Loss: 0.04209\n",
      "Epoch [1700/10000], Loss: 0.04203\n",
      "Epoch [1720/10000], Loss: 0.04196\n",
      "Epoch [1740/10000], Loss: 0.04189\n",
      "Epoch [1760/10000], Loss: 0.04182\n",
      "Epoch [1780/10000], Loss: 0.04175\n",
      "Epoch [1800/10000], Loss: 0.04168\n",
      "Epoch [1820/10000], Loss: 0.04162\n",
      "Epoch [1840/10000], Loss: 0.04155\n",
      "Epoch [1860/10000], Loss: 0.04148\n",
      "Epoch [1880/10000], Loss: 0.04141\n",
      "Epoch [1900/10000], Loss: 0.04135\n",
      "Epoch [1920/10000], Loss: 0.04128\n",
      "Epoch [1940/10000], Loss: 0.04121\n",
      "Epoch [1960/10000], Loss: 0.04114\n",
      "Epoch [1980/10000], Loss: 0.04108\n",
      "Epoch [2000/10000], Loss: 0.04101\n",
      "Epoch [2020/10000], Loss: 0.04094\n",
      "Epoch [2040/10000], Loss: 0.04087\n",
      "Epoch [2060/10000], Loss: 0.04081\n",
      "Epoch [2080/10000], Loss: 0.04074\n",
      "Epoch [2100/10000], Loss: 0.04067\n",
      "Epoch [2120/10000], Loss: 0.04061\n",
      "Epoch [2140/10000], Loss: 0.04054\n",
      "Epoch [2160/10000], Loss: 0.04047\n",
      "Epoch [2180/10000], Loss: 0.04040\n",
      "Epoch [2200/10000], Loss: 0.04034\n",
      "Epoch [2220/10000], Loss: 0.04027\n",
      "Epoch [2240/10000], Loss: 0.04020\n",
      "Epoch [2260/10000], Loss: 0.04014\n",
      "Epoch [2280/10000], Loss: 0.04007\n",
      "Epoch [2300/10000], Loss: 0.04000\n",
      "Epoch [2320/10000], Loss: 0.03994\n",
      "Epoch [2340/10000], Loss: 0.03987\n",
      "Epoch [2360/10000], Loss: 0.03981\n",
      "Epoch [2380/10000], Loss: 0.03974\n",
      "Epoch [2400/10000], Loss: 0.03967\n",
      "Epoch [2420/10000], Loss: 0.03961\n",
      "Epoch [2440/10000], Loss: 0.03954\n",
      "Epoch [2460/10000], Loss: 0.03948\n",
      "Epoch [2480/10000], Loss: 0.03941\n",
      "Epoch [2500/10000], Loss: 0.03934\n",
      "Epoch [2520/10000], Loss: 0.03928\n",
      "Epoch [2540/10000], Loss: 0.03921\n",
      "Epoch [2560/10000], Loss: 0.03915\n",
      "Epoch [2580/10000], Loss: 0.03908\n",
      "Epoch [2600/10000], Loss: 0.03902\n",
      "Epoch [2620/10000], Loss: 0.03895\n",
      "Epoch [2640/10000], Loss: 0.03888\n",
      "Epoch [2660/10000], Loss: 0.03882\n",
      "Epoch [2680/10000], Loss: 0.03875\n",
      "Epoch [2700/10000], Loss: 0.03869\n",
      "Epoch [2720/10000], Loss: 0.03862\n",
      "Epoch [2740/10000], Loss: 0.03856\n",
      "Epoch [2760/10000], Loss: 0.03849\n",
      "Epoch [2780/10000], Loss: 0.03843\n",
      "Epoch [2800/10000], Loss: 0.03836\n",
      "Epoch [2820/10000], Loss: 0.03830\n",
      "Epoch [2840/10000], Loss: 0.03823\n",
      "Epoch [2860/10000], Loss: 0.03817\n",
      "Epoch [2880/10000], Loss: 0.03810\n",
      "Epoch [2900/10000], Loss: 0.03804\n",
      "Epoch [2920/10000], Loss: 0.03797\n",
      "Epoch [2940/10000], Loss: 0.03791\n",
      "Epoch [2960/10000], Loss: 0.03785\n",
      "Epoch [2980/10000], Loss: 0.03778\n",
      "Epoch [3000/10000], Loss: 0.03772\n",
      "Epoch [3020/10000], Loss: 0.03765\n",
      "Epoch [3040/10000], Loss: 0.03759\n",
      "Epoch [3060/10000], Loss: 0.03752\n",
      "Epoch [3080/10000], Loss: 0.03746\n",
      "Epoch [3100/10000], Loss: 0.03740\n",
      "Epoch [3120/10000], Loss: 0.03733\n",
      "Epoch [3140/10000], Loss: 0.03727\n",
      "Epoch [3160/10000], Loss: 0.03720\n",
      "Epoch [3180/10000], Loss: 0.03714\n",
      "Epoch [3200/10000], Loss: 0.03707\n",
      "Epoch [3220/10000], Loss: 0.03701\n",
      "Epoch [3240/10000], Loss: 0.03695\n",
      "Epoch [3260/10000], Loss: 0.03688\n",
      "Epoch [3280/10000], Loss: 0.03682\n",
      "Epoch [3300/10000], Loss: 0.03676\n",
      "Epoch [3320/10000], Loss: 0.03669\n",
      "Epoch [3340/10000], Loss: 0.03663\n",
      "Epoch [3360/10000], Loss: 0.03657\n",
      "Epoch [3380/10000], Loss: 0.03650\n",
      "Epoch [3400/10000], Loss: 0.03644\n",
      "Epoch [3420/10000], Loss: 0.03638\n",
      "Epoch [3440/10000], Loss: 0.03631\n",
      "Epoch [3460/10000], Loss: 0.03625\n",
      "Epoch [3480/10000], Loss: 0.03619\n",
      "Epoch [3500/10000], Loss: 0.03612\n",
      "Epoch [3520/10000], Loss: 0.03606\n",
      "Epoch [3540/10000], Loss: 0.03600\n",
      "Epoch [3560/10000], Loss: 0.03593\n",
      "Epoch [3580/10000], Loss: 0.03587\n",
      "Epoch [3600/10000], Loss: 0.03581\n",
      "Epoch [3620/10000], Loss: 0.03575\n",
      "Epoch [3640/10000], Loss: 0.03568\n",
      "Epoch [3660/10000], Loss: 0.03562\n",
      "Epoch [3680/10000], Loss: 0.03556\n",
      "Epoch [3700/10000], Loss: 0.03549\n",
      "Epoch [3720/10000], Loss: 0.03543\n",
      "Epoch [3740/10000], Loss: 0.03537\n",
      "Epoch [3760/10000], Loss: 0.03531\n",
      "Epoch [3780/10000], Loss: 0.03525\n",
      "Epoch [3800/10000], Loss: 0.03518\n",
      "Epoch [3820/10000], Loss: 0.03512\n",
      "Epoch [3840/10000], Loss: 0.03506\n",
      "Epoch [3860/10000], Loss: 0.03500\n",
      "Epoch [3880/10000], Loss: 0.03493\n",
      "Epoch [3900/10000], Loss: 0.03487\n",
      "Epoch [3920/10000], Loss: 0.03481\n",
      "Epoch [3940/10000], Loss: 0.03475\n",
      "Epoch [3960/10000], Loss: 0.03469\n",
      "Epoch [3980/10000], Loss: 0.03462\n",
      "Epoch [4000/10000], Loss: 0.03456\n",
      "Epoch [4020/10000], Loss: 0.03450\n",
      "Epoch [4040/10000], Loss: 0.03444\n",
      "Epoch [4060/10000], Loss: 0.03438\n",
      "Epoch [4080/10000], Loss: 0.03432\n",
      "Epoch [4100/10000], Loss: 0.03425\n",
      "Epoch [4120/10000], Loss: 0.03419\n",
      "Epoch [4140/10000], Loss: 0.03413\n",
      "Epoch [4160/10000], Loss: 0.03407\n",
      "Epoch [4180/10000], Loss: 0.03401\n",
      "Epoch [4200/10000], Loss: 0.03395\n",
      "Epoch [4220/10000], Loss: 0.03389\n",
      "Epoch [4240/10000], Loss: 0.03382\n",
      "Epoch [4260/10000], Loss: 0.03376\n",
      "Epoch [4280/10000], Loss: 0.03370\n",
      "Epoch [4300/10000], Loss: 0.03364\n",
      "Epoch [4320/10000], Loss: 0.03358\n",
      "Epoch [4340/10000], Loss: 0.03352\n",
      "Epoch [4360/10000], Loss: 0.03346\n",
      "Epoch [4380/10000], Loss: 0.03340\n",
      "Epoch [4400/10000], Loss: 0.03334\n",
      "Epoch [4420/10000], Loss: 0.03328\n",
      "Epoch [4440/10000], Loss: 0.03321\n",
      "Epoch [4460/10000], Loss: 0.03315\n",
      "Epoch [4480/10000], Loss: 0.03309\n",
      "Epoch [4500/10000], Loss: 0.03303\n",
      "Epoch [4520/10000], Loss: 0.03297\n",
      "Epoch [4540/10000], Loss: 0.03291\n",
      "Epoch [4560/10000], Loss: 0.03285\n",
      "Epoch [4580/10000], Loss: 0.03279\n",
      "Epoch [4600/10000], Loss: 0.03273\n",
      "Epoch [4620/10000], Loss: 0.03267\n",
      "Epoch [4640/10000], Loss: 0.03261\n",
      "Epoch [4660/10000], Loss: 0.03255\n",
      "Epoch [4680/10000], Loss: 0.03249\n",
      "Epoch [4700/10000], Loss: 0.03243\n",
      "Epoch [4720/10000], Loss: 0.03237\n",
      "Epoch [4740/10000], Loss: 0.03231\n",
      "Epoch [4760/10000], Loss: 0.03225\n",
      "Epoch [4780/10000], Loss: 0.03219\n",
      "Epoch [4800/10000], Loss: 0.03213\n",
      "Epoch [4820/10000], Loss: 0.03207\n",
      "Epoch [4840/10000], Loss: 0.03201\n",
      "Epoch [4860/10000], Loss: 0.03195\n",
      "Epoch [4880/10000], Loss: 0.03189\n",
      "Epoch [4900/10000], Loss: 0.03183\n",
      "Epoch [4920/10000], Loss: 0.03177\n",
      "Epoch [4940/10000], Loss: 0.03171\n",
      "Epoch [4960/10000], Loss: 0.03165\n",
      "Epoch [4980/10000], Loss: 0.03159\n",
      "Epoch [5000/10000], Loss: 0.03154\n",
      "Epoch [5020/10000], Loss: 0.03148\n",
      "Epoch [5040/10000], Loss: 0.03142\n",
      "Epoch [5060/10000], Loss: 0.03136\n",
      "Epoch [5080/10000], Loss: 0.03130\n",
      "Epoch [5100/10000], Loss: 0.03124\n",
      "Epoch [5120/10000], Loss: 0.03118\n",
      "Epoch [5140/10000], Loss: 0.03112\n",
      "Epoch [5160/10000], Loss: 0.03106\n",
      "Epoch [5180/10000], Loss: 0.03100\n",
      "Epoch [5200/10000], Loss: 0.03094\n",
      "Epoch [5220/10000], Loss: 0.03088\n",
      "Epoch [5240/10000], Loss: 0.03083\n",
      "Epoch [5260/10000], Loss: 0.03077\n",
      "Epoch [5280/10000], Loss: 0.03071\n",
      "Epoch [5300/10000], Loss: 0.03065\n",
      "Epoch [5320/10000], Loss: 0.03059\n",
      "Epoch [5340/10000], Loss: 0.03053\n",
      "Epoch [5360/10000], Loss: 0.03047\n",
      "Epoch [5380/10000], Loss: 0.03042\n",
      "Epoch [5400/10000], Loss: 0.03036\n",
      "Epoch [5420/10000], Loss: 0.03030\n",
      "Epoch [5440/10000], Loss: 0.03024\n",
      "Epoch [5460/10000], Loss: 0.03018\n",
      "Epoch [5480/10000], Loss: 0.03012\n",
      "Epoch [5500/10000], Loss: 0.03007\n",
      "Epoch [5520/10000], Loss: 0.03001\n",
      "Epoch [5540/10000], Loss: 0.02995\n",
      "Epoch [5560/10000], Loss: 0.02989\n",
      "Epoch [5580/10000], Loss: 0.02983\n",
      "Epoch [5600/10000], Loss: 0.02978\n",
      "Epoch [5620/10000], Loss: 0.02972\n",
      "Epoch [5640/10000], Loss: 0.02966\n",
      "Epoch [5660/10000], Loss: 0.02960\n",
      "Epoch [5680/10000], Loss: 0.02954\n",
      "Epoch [5700/10000], Loss: 0.02949\n",
      "Epoch [5720/10000], Loss: 0.02943\n",
      "Epoch [5740/10000], Loss: 0.02937\n",
      "Epoch [5760/10000], Loss: 0.02931\n",
      "Epoch [5780/10000], Loss: 0.02926\n",
      "Epoch [5800/10000], Loss: 0.02920\n",
      "Epoch [5820/10000], Loss: 0.02914\n",
      "Epoch [5840/10000], Loss: 0.02908\n",
      "Epoch [5860/10000], Loss: 0.02903\n",
      "Epoch [5880/10000], Loss: 0.02897\n",
      "Epoch [5900/10000], Loss: 0.02891\n",
      "Epoch [5920/10000], Loss: 0.02885\n",
      "Epoch [5940/10000], Loss: 0.02880\n",
      "Epoch [5960/10000], Loss: 0.02874\n",
      "Epoch [5980/10000], Loss: 0.02868\n",
      "Epoch [6000/10000], Loss: 0.02863\n",
      "Epoch [6020/10000], Loss: 0.02857\n",
      "Epoch [6040/10000], Loss: 0.02851\n",
      "Epoch [6060/10000], Loss: 0.02846\n",
      "Epoch [6080/10000], Loss: 0.02840\n",
      "Epoch [6100/10000], Loss: 0.02834\n",
      "Epoch [6120/10000], Loss: 0.02828\n",
      "Epoch [6140/10000], Loss: 0.02823\n",
      "Epoch [6160/10000], Loss: 0.02817\n",
      "Epoch [6180/10000], Loss: 0.02811\n",
      "Epoch [6200/10000], Loss: 0.02806\n",
      "Epoch [6220/10000], Loss: 0.02800\n",
      "Epoch [6240/10000], Loss: 0.02795\n",
      "Epoch [6260/10000], Loss: 0.02789\n",
      "Epoch [6280/10000], Loss: 0.02783\n",
      "Epoch [6300/10000], Loss: 0.02778\n",
      "Epoch [6320/10000], Loss: 0.02772\n",
      "Epoch [6340/10000], Loss: 0.02766\n",
      "Epoch [6360/10000], Loss: 0.02761\n",
      "Epoch [6380/10000], Loss: 0.02755\n",
      "Epoch [6400/10000], Loss: 0.02750\n",
      "Epoch [6420/10000], Loss: 0.02744\n",
      "Epoch [6440/10000], Loss: 0.02738\n",
      "Epoch [6460/10000], Loss: 0.02733\n",
      "Epoch [6480/10000], Loss: 0.02727\n",
      "Epoch [6500/10000], Loss: 0.02722\n",
      "Epoch [6520/10000], Loss: 0.02716\n",
      "Epoch [6540/10000], Loss: 0.02711\n",
      "Epoch [6560/10000], Loss: 0.02705\n",
      "Epoch [6580/10000], Loss: 0.02700\n",
      "Epoch [6600/10000], Loss: 0.02694\n",
      "Epoch [6620/10000], Loss: 0.02688\n",
      "Epoch [6640/10000], Loss: 0.02683\n",
      "Epoch [6660/10000], Loss: 0.02677\n",
      "Epoch [6680/10000], Loss: 0.02672\n",
      "Epoch [6700/10000], Loss: 0.02666\n",
      "Epoch [6720/10000], Loss: 0.02661\n",
      "Epoch [6740/10000], Loss: 0.02655\n",
      "Epoch [6760/10000], Loss: 0.02650\n",
      "Epoch [6780/10000], Loss: 0.02644\n",
      "Epoch [6800/10000], Loss: 0.02639\n",
      "Epoch [6820/10000], Loss: 0.02633\n",
      "Epoch [6840/10000], Loss: 0.02628\n",
      "Epoch [6860/10000], Loss: 0.02622\n",
      "Epoch [6880/10000], Loss: 0.02617\n",
      "Epoch [6900/10000], Loss: 0.02611\n",
      "Epoch [6920/10000], Loss: 0.02606\n",
      "Epoch [6940/10000], Loss: 0.02600\n",
      "Epoch [6960/10000], Loss: 0.02595\n",
      "Epoch [6980/10000], Loss: 0.02589\n",
      "Epoch [7000/10000], Loss: 0.02584\n",
      "Epoch [7020/10000], Loss: 0.02579\n",
      "Epoch [7040/10000], Loss: 0.02573\n",
      "Epoch [7060/10000], Loss: 0.02568\n",
      "Epoch [7080/10000], Loss: 0.02562\n",
      "Epoch [7100/10000], Loss: 0.02557\n",
      "Epoch [7120/10000], Loss: 0.02551\n",
      "Epoch [7140/10000], Loss: 0.02546\n",
      "Epoch [7160/10000], Loss: 0.02541\n",
      "Epoch [7180/10000], Loss: 0.02535\n",
      "Epoch [7200/10000], Loss: 0.02530\n",
      "Epoch [7220/10000], Loss: 0.02524\n",
      "Epoch [7240/10000], Loss: 0.02519\n",
      "Epoch [7260/10000], Loss: 0.02514\n",
      "Epoch [7280/10000], Loss: 0.02508\n",
      "Epoch [7300/10000], Loss: 0.02503\n",
      "Epoch [7320/10000], Loss: 0.02497\n",
      "Epoch [7340/10000], Loss: 0.02492\n",
      "Epoch [7360/10000], Loss: 0.02487\n",
      "Epoch [7380/10000], Loss: 0.02481\n",
      "Epoch [7400/10000], Loss: 0.02476\n",
      "Epoch [7420/10000], Loss: 0.02471\n",
      "Epoch [7440/10000], Loss: 0.02465\n",
      "Epoch [7460/10000], Loss: 0.02460\n",
      "Epoch [7480/10000], Loss: 0.02455\n",
      "Epoch [7500/10000], Loss: 0.02449\n",
      "Epoch [7520/10000], Loss: 0.02444\n",
      "Epoch [7540/10000], Loss: 0.02439\n",
      "Epoch [7560/10000], Loss: 0.02433\n",
      "Epoch [7580/10000], Loss: 0.02428\n",
      "Epoch [7600/10000], Loss: 0.02423\n",
      "Epoch [7620/10000], Loss: 0.02417\n",
      "Epoch [7640/10000], Loss: 0.02412\n",
      "Epoch [7660/10000], Loss: 0.02407\n",
      "Epoch [7680/10000], Loss: 0.02402\n",
      "Epoch [7700/10000], Loss: 0.02396\n",
      "Epoch [7720/10000], Loss: 0.02391\n",
      "Epoch [7740/10000], Loss: 0.02386\n",
      "Epoch [7760/10000], Loss: 0.02381\n",
      "Epoch [7780/10000], Loss: 0.02375\n",
      "Epoch [7800/10000], Loss: 0.02370\n",
      "Epoch [7820/10000], Loss: 0.02365\n",
      "Epoch [7840/10000], Loss: 0.02360\n",
      "Epoch [7860/10000], Loss: 0.02354\n",
      "Epoch [7880/10000], Loss: 0.02349\n",
      "Epoch [7900/10000], Loss: 0.02344\n",
      "Epoch [7920/10000], Loss: 0.02339\n",
      "Epoch [7940/10000], Loss: 0.02333\n",
      "Epoch [7960/10000], Loss: 0.02328\n",
      "Epoch [7980/10000], Loss: 0.02323\n",
      "Epoch [8000/10000], Loss: 0.02318\n",
      "Epoch [8020/10000], Loss: 0.02313\n",
      "Epoch [8040/10000], Loss: 0.02307\n",
      "Epoch [8060/10000], Loss: 0.02302\n",
      "Epoch [8080/10000], Loss: 0.02297\n",
      "Epoch [8100/10000], Loss: 0.02292\n",
      "Epoch [8120/10000], Loss: 0.02287\n",
      "Epoch [8140/10000], Loss: 0.02282\n",
      "Epoch [8160/10000], Loss: 0.02276\n",
      "Epoch [8180/10000], Loss: 0.02271\n",
      "Epoch [8200/10000], Loss: 0.02266\n",
      "Epoch [8220/10000], Loss: 0.02261\n",
      "Epoch [8240/10000], Loss: 0.02256\n",
      "Epoch [8260/10000], Loss: 0.02251\n",
      "Epoch [8280/10000], Loss: 0.02246\n",
      "Epoch [8300/10000], Loss: 0.02240\n",
      "Epoch [8320/10000], Loss: 0.02235\n",
      "Epoch [8340/10000], Loss: 0.02230\n",
      "Epoch [8360/10000], Loss: 0.02225\n",
      "Epoch [8380/10000], Loss: 0.02220\n",
      "Epoch [8400/10000], Loss: 0.02215\n",
      "Epoch [8420/10000], Loss: 0.02210\n",
      "Epoch [8440/10000], Loss: 0.02205\n",
      "Epoch [8460/10000], Loss: 0.02200\n",
      "Epoch [8480/10000], Loss: 0.02195\n",
      "Epoch [8500/10000], Loss: 0.02190\n",
      "Epoch [8520/10000], Loss: 0.02184\n",
      "Epoch [8540/10000], Loss: 0.02179\n",
      "Epoch [8560/10000], Loss: 0.02174\n",
      "Epoch [8580/10000], Loss: 0.02169\n",
      "Epoch [8600/10000], Loss: 0.02164\n",
      "Epoch [8620/10000], Loss: 0.02159\n",
      "Epoch [8640/10000], Loss: 0.02154\n",
      "Epoch [8660/10000], Loss: 0.02149\n",
      "Epoch [8680/10000], Loss: 0.02144\n",
      "Epoch [8700/10000], Loss: 0.02139\n",
      "Epoch [8720/10000], Loss: 0.02134\n",
      "Epoch [8740/10000], Loss: 0.02129\n",
      "Epoch [8760/10000], Loss: 0.02124\n",
      "Epoch [8780/10000], Loss: 0.02119\n",
      "Epoch [8800/10000], Loss: 0.02114\n",
      "Epoch [8820/10000], Loss: 0.02109\n",
      "Epoch [8840/10000], Loss: 0.02104\n",
      "Epoch [8860/10000], Loss: 0.02099\n",
      "Epoch [8880/10000], Loss: 0.02094\n",
      "Epoch [8900/10000], Loss: 0.02089\n",
      "Epoch [8920/10000], Loss: 0.02084\n",
      "Epoch [8940/10000], Loss: 0.02079\n",
      "Epoch [8960/10000], Loss: 0.02074\n",
      "Epoch [8980/10000], Loss: 0.02070\n",
      "Epoch [9000/10000], Loss: 0.02065\n",
      "Epoch [9020/10000], Loss: 0.02060\n",
      "Epoch [9040/10000], Loss: 0.02055\n",
      "Epoch [9060/10000], Loss: 0.02050\n",
      "Epoch [9080/10000], Loss: 0.02045\n",
      "Epoch [9100/10000], Loss: 0.02040\n",
      "Epoch [9120/10000], Loss: 0.02035\n",
      "Epoch [9140/10000], Loss: 0.02030\n",
      "Epoch [9160/10000], Loss: 0.02025\n",
      "Epoch [9180/10000], Loss: 0.02020\n",
      "Epoch [9200/10000], Loss: 0.02016\n",
      "Epoch [9220/10000], Loss: 0.02011\n",
      "Epoch [9240/10000], Loss: 0.02006\n",
      "Epoch [9260/10000], Loss: 0.02001\n",
      "Epoch [9280/10000], Loss: 0.01996\n",
      "Epoch [9300/10000], Loss: 0.01991\n",
      "Epoch [9320/10000], Loss: 0.01986\n",
      "Epoch [9340/10000], Loss: 0.01982\n",
      "Epoch [9360/10000], Loss: 0.01977\n",
      "Epoch [9380/10000], Loss: 0.01972\n",
      "Epoch [9400/10000], Loss: 0.01967\n",
      "Epoch [9420/10000], Loss: 0.01962\n",
      "Epoch [9440/10000], Loss: 0.01957\n",
      "Epoch [9460/10000], Loss: 0.01953\n",
      "Epoch [9480/10000], Loss: 0.01948\n",
      "Epoch [9500/10000], Loss: 0.01943\n",
      "Epoch [9520/10000], Loss: 0.01938\n",
      "Epoch [9540/10000], Loss: 0.01933\n",
      "Epoch [9560/10000], Loss: 0.01929\n",
      "Epoch [9580/10000], Loss: 0.01924\n",
      "Epoch [9600/10000], Loss: 0.01919\n",
      "Epoch [9620/10000], Loss: 0.01914\n",
      "Epoch [9640/10000], Loss: 0.01910\n",
      "Epoch [9660/10000], Loss: 0.01905\n",
      "Epoch [9680/10000], Loss: 0.01900\n",
      "Epoch [9700/10000], Loss: 0.01895\n",
      "Epoch [9720/10000], Loss: 0.01891\n",
      "Epoch [9740/10000], Loss: 0.01886\n",
      "Epoch [9760/10000], Loss: 0.01881\n",
      "Epoch [9780/10000], Loss: 0.01876\n",
      "Epoch [9800/10000], Loss: 0.01872\n",
      "Epoch [9820/10000], Loss: 0.01867\n",
      "Epoch [9840/10000], Loss: 0.01862\n",
      "Epoch [9860/10000], Loss: 0.01858\n",
      "Epoch [9880/10000], Loss: 0.01853\n",
      "Epoch [9900/10000], Loss: 0.01848\n",
      "Epoch [9920/10000], Loss: 0.01844\n",
      "Epoch [9940/10000], Loss: 0.01839\n",
      "Epoch [9960/10000], Loss: 0.01834\n",
      "Epoch [9980/10000], Loss: 0.01830\n"
     ]
    }
   ],
   "source": [
    "MAX = 10000\n",
    "for epoch in range(MAX):\n",
    "    u_pred_pde = u_net(x_batch,t_batch)\n",
    "    # du/dt\n",
    "    # du_dt = torch.autograd.grad(u_pred_pde, inputs=t_batch, grad_outputs=torch.ones_like(t_batch), create_graph=True)[0]\n",
    "    # du/dx, du/dt\n",
    "    [du_dx, du_dt] = torch.autograd.grad(u_pred_pde, inputs=[x_batch, t_batch], grad_outputs=torch.ones_like(x_batch), create_graph=True, retain_graph=True)\n",
    "    # d2u/dx2\n",
    "    [d2u_dx2, du_dxdt] = torch.autograd.grad(du_dx, inputs=[x_batch, t_batch], grad_outputs=torch.ones_like(x_batch), create_graph=True, retain_graph=True)\n",
    "    # loss pde\n",
    "    loss_pde = torch.mean((du_dt[0] + ka*2*torch.pi*torch.sin(2*torch.pi*x_batch)*du_dx[0] - (ka*torch.cos(2*torch.pi*x_batch) +kb)*d2u_dx2[0])**2)\n",
    "    #loss_pde = torch.mean((du_dt - 0.01*d2u_dx2[0])**2)\n",
    "    # стираем предыдущий граф градиента\n",
    "    optimizer_pde.zero_grad()\n",
    "    # обратное распространение ошибки\n",
    "    loss_pde.backward()\n",
    "    # записываем новые коэфициента\n",
    "    optimizer_pde.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch [{epoch}/{MAX}], Loss: {loss_pde.item():.5f}')\n",
    "    if loss_pde < 0.0001:\n",
    "        print('stop pde optimization')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "acfe54f2-6f69-4757-9f62-41a215f14890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2290],\n",
       "        [0.2356],\n",
       "        [0.2421],\n",
       "        [0.2488],\n",
       "        [0.2554],\n",
       "        [0.2621],\n",
       "        [0.2687],\n",
       "        [0.2753],\n",
       "        [0.2818],\n",
       "        [0.2883],\n",
       "        [0.5011],\n",
       "        [0.5097],\n",
       "        [0.5181],\n",
       "        [0.5262],\n",
       "        [0.5340],\n",
       "        [0.5415],\n",
       "        [0.5487],\n",
       "        [0.5555],\n",
       "        [0.5620],\n",
       "        [0.5680],\n",
       "        [0.7381],\n",
       "        [0.7466],\n",
       "        [0.7545],\n",
       "        [0.7620],\n",
       "        [0.7690],\n",
       "        [0.7754],\n",
       "        [0.7814],\n",
       "        [0.7867],\n",
       "        [0.7915],\n",
       "        [0.7958],\n",
       "        [0.8981],\n",
       "        [0.9043],\n",
       "        [0.9100],\n",
       "        [0.9151],\n",
       "        [0.9196],\n",
       "        [0.9236],\n",
       "        [0.9270],\n",
       "        [0.9298],\n",
       "        [0.9320],\n",
       "        [0.9337],\n",
       "        [0.9706],\n",
       "        [0.9740],\n",
       "        [0.9768],\n",
       "        [0.9791],\n",
       "        [0.9808],\n",
       "        [0.9820],\n",
       "        [0.9826],\n",
       "        [0.9828],\n",
       "        [0.9824],\n",
       "        [0.9816],\n",
       "        [0.9622],\n",
       "        [0.9630],\n",
       "        [0.9634],\n",
       "        [0.9632],\n",
       "        [0.9625],\n",
       "        [0.9614],\n",
       "        [0.9598],\n",
       "        [0.9578],\n",
       "        [0.9553],\n",
       "        [0.9524],\n",
       "        [0.8810],\n",
       "        [0.8801],\n",
       "        [0.8787],\n",
       "        [0.8769],\n",
       "        [0.8746],\n",
       "        [0.8720],\n",
       "        [0.8689],\n",
       "        [0.8654],\n",
       "        [0.8616],\n",
       "        [0.8574],\n",
       "        [0.7334],\n",
       "        [0.7317],\n",
       "        [0.7295],\n",
       "        [0.7270],\n",
       "        [0.7241],\n",
       "        [0.7208],\n",
       "        [0.7171],\n",
       "        [0.7132],\n",
       "        [0.7088],\n",
       "        [0.7042],\n",
       "        [0.5271],\n",
       "        [0.5254],\n",
       "        [0.5234],\n",
       "        [0.5210],\n",
       "        [0.5183],\n",
       "        [0.5154],\n",
       "        [0.5121],\n",
       "        [0.5086],\n",
       "        [0.5048],\n",
       "        [0.5007],\n",
       "        [0.2761],\n",
       "        [0.2750],\n",
       "        [0.2737],\n",
       "        [0.2721],\n",
       "        [0.2704],\n",
       "        [0.2684],\n",
       "        [0.2663],\n",
       "        [0.2640],\n",
       "        [0.2615],\n",
       "        [0.2588]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net(x_batch,t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9b13f-1e64-4174-8b9d-90548b76ed34",
   "metadata": {},
   "source": [
    "## 1.5.2 Лосс Initial condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3ea9f-3056-485b-9564-ba1d6d6a32ab",
   "metadata": {},
   "source": [
    "$$ L_{IC} = \\frac{1}{K} \\sum_{j=1}^K || N_u(x,0) - sin(\\pi * x) ||^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83779be4-f5fa-49cc-90b6-c7d2f28bce05",
   "metadata": {},
   "source": [
    "### $ x \\in [0,1] \\, , t=0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "7bbb9e92-c287-485a-ab7a-07ea69c27118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_net = Nu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "65ba242e-898c-454a-bf9e-59fc2e146414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.1111, 0.0000],\n",
       "        [0.2222, 0.0000],\n",
       "        [0.3333, 0.0000],\n",
       "        [0.4444, 0.0000],\n",
       "        [0.5556, 0.0000],\n",
       "        [0.6667, 0.0000],\n",
       "        [0.7778, 0.0000],\n",
       "        [0.8889, 0.0000],\n",
       "        [1.0000, 0.0000]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = torch.linspace(0,1,10).view(-1,1)\n",
    "x_batch.requires_grad = True\n",
    "\n",
    "t_batch = torch.zeros(10).view(-1,1)\n",
    "t_batch.requires_grad = True\n",
    "\n",
    "torch.cat([x_batch, t_batch], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "e61924bf-e325-4455-be92-32db26dde503",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ic = torch.optim.Adam( u_net.parameters(), lr=1e-3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "f27f620d-8170-404b-b9a8-060e6e438a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], Loss: 0.23432\n",
      "Epoch [20/10000], Loss: 0.13250\n",
      "Epoch [40/10000], Loss: 0.09610\n",
      "Epoch [60/10000], Loss: 0.07531\n",
      "Epoch [80/10000], Loss: 0.05816\n",
      "Epoch [100/10000], Loss: 0.04466\n",
      "Epoch [120/10000], Loss: 0.03419\n",
      "Epoch [140/10000], Loss: 0.02623\n",
      "Epoch [160/10000], Loss: 0.02028\n",
      "Epoch [180/10000], Loss: 0.01587\n",
      "Epoch [200/10000], Loss: 0.01263\n",
      "Epoch [220/10000], Loss: 0.01026\n",
      "Epoch [240/10000], Loss: 0.00852\n",
      "Epoch [260/10000], Loss: 0.00723\n",
      "Epoch [280/10000], Loss: 0.00625\n",
      "Epoch [300/10000], Loss: 0.00550\n",
      "Epoch [320/10000], Loss: 0.00490\n",
      "Epoch [340/10000], Loss: 0.00441\n",
      "Epoch [360/10000], Loss: 0.00400\n",
      "Epoch [380/10000], Loss: 0.00365\n",
      "Epoch [400/10000], Loss: 0.00333\n",
      "Epoch [420/10000], Loss: 0.00305\n",
      "Epoch [440/10000], Loss: 0.00280\n",
      "Epoch [460/10000], Loss: 0.00258\n",
      "Epoch [480/10000], Loss: 0.00237\n",
      "Epoch [500/10000], Loss: 0.00218\n",
      "Epoch [520/10000], Loss: 0.00201\n",
      "Epoch [540/10000], Loss: 0.00185\n",
      "Epoch [560/10000], Loss: 0.00171\n",
      "Epoch [580/10000], Loss: 0.00158\n",
      "Epoch [600/10000], Loss: 0.00146\n",
      "Epoch [620/10000], Loss: 0.00135\n",
      "Epoch [640/10000], Loss: 0.00125\n",
      "Epoch [660/10000], Loss: 0.00116\n",
      "Epoch [680/10000], Loss: 0.00108\n",
      "Epoch [700/10000], Loss: 0.00101\n",
      "Epoch [720/10000], Loss: 0.00094\n",
      "Epoch [740/10000], Loss: 0.00088\n",
      "Epoch [760/10000], Loss: 0.00082\n",
      "Epoch [780/10000], Loss: 0.00077\n",
      "Epoch [800/10000], Loss: 0.00073\n",
      "Epoch [820/10000], Loss: 0.00068\n",
      "Epoch [840/10000], Loss: 0.00065\n",
      "Epoch [860/10000], Loss: 0.00061\n",
      "Epoch [880/10000], Loss: 0.00058\n",
      "Epoch [900/10000], Loss: 0.00055\n",
      "Epoch [920/10000], Loss: 0.00053\n",
      "Epoch [940/10000], Loss: 0.00050\n",
      "Epoch [960/10000], Loss: 0.00048\n",
      "Epoch [980/10000], Loss: 0.00046\n",
      "Epoch [1000/10000], Loss: 0.00045\n",
      "Epoch [1020/10000], Loss: 0.00043\n",
      "Epoch [1040/10000], Loss: 0.00042\n",
      "Epoch [1060/10000], Loss: 0.00040\n",
      "Epoch [1080/10000], Loss: 0.00039\n",
      "Epoch [1100/10000], Loss: 0.00038\n",
      "Epoch [1120/10000], Loss: 0.00037\n",
      "Epoch [1140/10000], Loss: 0.00036\n",
      "Epoch [1160/10000], Loss: 0.00035\n",
      "Epoch [1180/10000], Loss: 0.00034\n",
      "Epoch [1200/10000], Loss: 0.00034\n",
      "Epoch [1220/10000], Loss: 0.00033\n",
      "Epoch [1240/10000], Loss: 0.00032\n",
      "Epoch [1260/10000], Loss: 0.00032\n",
      "Epoch [1280/10000], Loss: 0.00031\n",
      "Epoch [1300/10000], Loss: 0.00031\n",
      "Epoch [1320/10000], Loss: 0.00030\n",
      "Epoch [1340/10000], Loss: 0.00030\n",
      "Epoch [1360/10000], Loss: 0.00029\n",
      "Epoch [1380/10000], Loss: 0.00029\n",
      "Epoch [1400/10000], Loss: 0.00029\n",
      "Epoch [1420/10000], Loss: 0.00028\n",
      "Epoch [1440/10000], Loss: 0.00028\n",
      "Epoch [1460/10000], Loss: 0.00028\n",
      "Epoch [1480/10000], Loss: 0.00028\n",
      "Epoch [1500/10000], Loss: 0.00027\n",
      "Epoch [1520/10000], Loss: 0.00027\n",
      "Epoch [1540/10000], Loss: 0.00027\n",
      "Epoch [1560/10000], Loss: 0.00027\n",
      "Epoch [1580/10000], Loss: 0.00026\n",
      "Epoch [1600/10000], Loss: 0.00026\n",
      "Epoch [1620/10000], Loss: 0.00026\n",
      "Epoch [1640/10000], Loss: 0.00026\n",
      "Epoch [1660/10000], Loss: 0.00026\n",
      "Epoch [1680/10000], Loss: 0.00025\n",
      "Epoch [1700/10000], Loss: 0.00025\n",
      "Epoch [1720/10000], Loss: 0.00025\n",
      "Epoch [1740/10000], Loss: 0.00025\n",
      "Epoch [1760/10000], Loss: 0.00025\n",
      "Epoch [1780/10000], Loss: 0.00025\n",
      "Epoch [1800/10000], Loss: 0.00024\n",
      "Epoch [1820/10000], Loss: 0.00024\n",
      "Epoch [1840/10000], Loss: 0.00024\n",
      "Epoch [1860/10000], Loss: 0.00024\n",
      "Epoch [1880/10000], Loss: 0.00024\n",
      "Epoch [1900/10000], Loss: 0.00024\n",
      "Epoch [1920/10000], Loss: 0.00024\n",
      "Epoch [1940/10000], Loss: 0.00023\n",
      "Epoch [1960/10000], Loss: 0.00023\n",
      "Epoch [1980/10000], Loss: 0.00023\n",
      "Epoch [2000/10000], Loss: 0.00023\n",
      "Epoch [2020/10000], Loss: 0.00023\n",
      "Epoch [2040/10000], Loss: 0.00023\n",
      "Epoch [2060/10000], Loss: 0.00023\n",
      "Epoch [2080/10000], Loss: 0.00023\n",
      "Epoch [2100/10000], Loss: 0.00022\n",
      "Epoch [2120/10000], Loss: 0.00022\n",
      "Epoch [2140/10000], Loss: 0.00022\n",
      "Epoch [2160/10000], Loss: 0.00022\n",
      "Epoch [2180/10000], Loss: 0.00022\n",
      "Epoch [2200/10000], Loss: 0.00022\n",
      "Epoch [2220/10000], Loss: 0.00022\n",
      "Epoch [2240/10000], Loss: 0.00022\n",
      "Epoch [2260/10000], Loss: 0.00022\n",
      "Epoch [2280/10000], Loss: 0.00021\n",
      "Epoch [2300/10000], Loss: 0.00021\n",
      "Epoch [2320/10000], Loss: 0.00021\n",
      "Epoch [2340/10000], Loss: 0.00021\n",
      "Epoch [2360/10000], Loss: 0.00021\n",
      "Epoch [2380/10000], Loss: 0.00021\n",
      "Epoch [2400/10000], Loss: 0.00021\n",
      "Epoch [2420/10000], Loss: 0.00021\n",
      "Epoch [2440/10000], Loss: 0.00021\n",
      "Epoch [2460/10000], Loss: 0.00020\n",
      "Epoch [2480/10000], Loss: 0.00020\n",
      "Epoch [2500/10000], Loss: 0.00020\n",
      "Epoch [2520/10000], Loss: 0.00020\n",
      "Epoch [2540/10000], Loss: 0.00020\n",
      "Epoch [2560/10000], Loss: 0.00020\n",
      "Epoch [2580/10000], Loss: 0.00020\n",
      "Epoch [2600/10000], Loss: 0.00020\n",
      "Epoch [2620/10000], Loss: 0.00020\n",
      "Epoch [2640/10000], Loss: 0.00019\n",
      "Epoch [2660/10000], Loss: 0.00019\n",
      "Epoch [2680/10000], Loss: 0.00019\n",
      "Epoch [2700/10000], Loss: 0.00019\n",
      "Epoch [2720/10000], Loss: 0.00019\n",
      "Epoch [2740/10000], Loss: 0.00019\n",
      "Epoch [2760/10000], Loss: 0.00019\n",
      "Epoch [2780/10000], Loss: 0.00019\n",
      "Epoch [2800/10000], Loss: 0.00019\n",
      "Epoch [2820/10000], Loss: 0.00018\n",
      "Epoch [2840/10000], Loss: 0.00018\n",
      "Epoch [2860/10000], Loss: 0.00018\n",
      "Epoch [2880/10000], Loss: 0.00018\n",
      "Epoch [2900/10000], Loss: 0.00018\n",
      "Epoch [2920/10000], Loss: 0.00018\n",
      "Epoch [2940/10000], Loss: 0.00018\n",
      "Epoch [2960/10000], Loss: 0.00018\n",
      "Epoch [2980/10000], Loss: 0.00018\n",
      "Epoch [3000/10000], Loss: 0.00017\n",
      "Epoch [3020/10000], Loss: 0.00017\n",
      "Epoch [3040/10000], Loss: 0.00017\n",
      "Epoch [3060/10000], Loss: 0.00017\n",
      "Epoch [3080/10000], Loss: 0.00017\n",
      "Epoch [3100/10000], Loss: 0.00017\n",
      "Epoch [3120/10000], Loss: 0.00017\n",
      "Epoch [3140/10000], Loss: 0.00017\n",
      "Epoch [3160/10000], Loss: 0.00016\n",
      "Epoch [3180/10000], Loss: 0.00016\n",
      "Epoch [3200/10000], Loss: 0.00016\n",
      "Epoch [3220/10000], Loss: 0.00016\n",
      "Epoch [3240/10000], Loss: 0.00016\n",
      "Epoch [3260/10000], Loss: 0.00016\n",
      "Epoch [3280/10000], Loss: 0.00016\n",
      "Epoch [3300/10000], Loss: 0.00016\n",
      "Epoch [3320/10000], Loss: 0.00016\n",
      "Epoch [3340/10000], Loss: 0.00015\n",
      "Epoch [3360/10000], Loss: 0.00015\n",
      "Epoch [3380/10000], Loss: 0.00015\n",
      "Epoch [3400/10000], Loss: 0.00015\n",
      "Epoch [3420/10000], Loss: 0.00015\n",
      "Epoch [3440/10000], Loss: 0.00015\n",
      "Epoch [3460/10000], Loss: 0.00015\n",
      "Epoch [3480/10000], Loss: 0.00015\n",
      "Epoch [3500/10000], Loss: 0.00015\n",
      "Epoch [3520/10000], Loss: 0.00014\n",
      "Epoch [3540/10000], Loss: 0.00014\n",
      "Epoch [3560/10000], Loss: 0.00014\n",
      "Epoch [3580/10000], Loss: 0.00014\n",
      "Epoch [3600/10000], Loss: 0.00014\n",
      "Epoch [3620/10000], Loss: 0.00014\n",
      "Epoch [3640/10000], Loss: 0.00014\n",
      "Epoch [3660/10000], Loss: 0.00014\n",
      "Epoch [3680/10000], Loss: 0.00014\n",
      "Epoch [3700/10000], Loss: 0.00013\n",
      "Epoch [3720/10000], Loss: 0.00013\n",
      "Epoch [3740/10000], Loss: 0.00013\n",
      "Epoch [3760/10000], Loss: 0.00013\n",
      "Epoch [3780/10000], Loss: 0.00013\n",
      "Epoch [3800/10000], Loss: 0.00013\n",
      "Epoch [3820/10000], Loss: 0.00013\n",
      "Epoch [3840/10000], Loss: 0.00013\n",
      "Epoch [3860/10000], Loss: 0.00013\n",
      "Epoch [3880/10000], Loss: 0.00012\n",
      "Epoch [3900/10000], Loss: 0.00012\n",
      "Epoch [3920/10000], Loss: 0.00012\n",
      "Epoch [3940/10000], Loss: 0.00012\n",
      "Epoch [3960/10000], Loss: 0.00012\n",
      "Epoch [3980/10000], Loss: 0.00012\n",
      "Epoch [4000/10000], Loss: 0.00012\n",
      "Epoch [4020/10000], Loss: 0.00012\n",
      "Epoch [4040/10000], Loss: 0.00012\n",
      "Epoch [4060/10000], Loss: 0.00012\n",
      "Epoch [4080/10000], Loss: 0.00011\n",
      "Epoch [4100/10000], Loss: 0.00011\n",
      "Epoch [4120/10000], Loss: 0.00011\n",
      "Epoch [4140/10000], Loss: 0.00011\n",
      "Epoch [4160/10000], Loss: 0.00011\n",
      "Epoch [4180/10000], Loss: 0.00011\n",
      "Epoch [4200/10000], Loss: 0.00011\n",
      "Epoch [4220/10000], Loss: 0.00011\n",
      "Epoch [4240/10000], Loss: 0.00011\n",
      "Epoch [4260/10000], Loss: 0.00011\n",
      "Epoch [4280/10000], Loss: 0.00010\n",
      "Epoch [4300/10000], Loss: 0.00010\n",
      "Epoch [4320/10000], Loss: 0.00010\n",
      "Epoch [4340/10000], Loss: 0.00010\n",
      "Epoch [4360/10000], Loss: 0.00010\n",
      "stop initial condition optimization\n"
     ]
    }
   ],
   "source": [
    "MAX = 10000\n",
    "for epoch in range(MAX):\n",
    "    u_pred_ic = u_net(x_batch,t_batch)\n",
    "    # loss\n",
    "    loss_ic = torch.mean( (u_pred_ic - torch.sin(torch.pi*x_batch))**2 )\n",
    "    # стираем предыдущий граф градиента\n",
    "    optimizer_ic.zero_grad()\n",
    "    # обратное распространение ошибки\n",
    "    loss_ic.backward()\n",
    "    # записываем новые коэфициента\n",
    "    optimizer_ic.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch [{epoch}/{MAX}], Loss: {loss_ic.item():.5f}')\n",
    "    if loss_ic < 0.0001:\n",
    "        print('stop initial condition optimization')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "64af915d-5a00-4592-bbe9-366f76bda711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0125],\n",
       "        [0.3216],\n",
       "        [0.6432],\n",
       "        [0.8807],\n",
       "        [0.9908],\n",
       "        [0.9785],\n",
       "        [0.8573],\n",
       "        [0.6388],\n",
       "        [0.3425],\n",
       "        [0.0064]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net(x_batch,t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13f057-d18b-4708-b2a9-e9640faadfd2",
   "metadata": {},
   "source": [
    "## 1.5.3 Лосс Boundary condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac31fa4-fc53-416b-b9de-24080471e749",
   "metadata": {},
   "source": [
    "$$ L_{BC} = \\frac{1}{P} (\\sum_{j=1}^P || N_u(0,t) ||^2 + \\sum_{j=1}^P || N_u(L,t) ||^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87cf03-28e8-4b79-91a6-62d9cb36b6f7",
   "metadata": {},
   "source": [
    "### $ x \\in \\{0,1\\} \\, , t \\in (0,1] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "bfcc8d55-e1c6-4df5-a681-0d5af4aac2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_net = Nu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "265706da-7582-4814-bf46-d205c4faef49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1000],\n",
       "        [0.0000, 0.2000],\n",
       "        [0.0000, 0.3000],\n",
       "        [0.0000, 0.4000],\n",
       "        [0.0000, 0.5000],\n",
       "        [0.0000, 0.6000],\n",
       "        [0.0000, 0.7000],\n",
       "        [0.0000, 0.8000],\n",
       "        [0.0000, 0.9000],\n",
       "        [0.0000, 1.0000],\n",
       "        [1.0000, 0.1000],\n",
       "        [1.0000, 0.2000],\n",
       "        [1.0000, 0.3000],\n",
       "        [1.0000, 0.4000],\n",
       "        [1.0000, 0.5000],\n",
       "        [1.0000, 0.6000],\n",
       "        [1.0000, 0.7000],\n",
       "        [1.0000, 0.8000],\n",
       "        [1.0000, 0.9000],\n",
       "        [1.0000, 1.0000]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = torch.cat( [torch.zeros(10).view(-1,1), torch.ones(10).view(-1,1)])\n",
    "x_batch.requires_grad = True\n",
    "\n",
    "t_batch = torch.cat( [torch.linspace(0,1,11)[1:11].view(-1,1), torch.linspace(0,1,11)[1:11].view(-1,1)] )\n",
    "t_batch.requires_grad = True\n",
    "\n",
    "torch.cat([x_batch, t_batch], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "d492bf86-38aa-4d37-85d7-4d8ee741ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_bc = torch.optim.Adam( u_net.parameters(), lr=1e-3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "95911775-b93b-4029-b425-35de44b8734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], Loss: 0.00212\n",
      "Epoch [20/10000], Loss: 0.00034\n",
      "Epoch [40/10000], Loss: 0.00022\n",
      "Epoch [60/10000], Loss: 0.00017\n",
      "Epoch [80/10000], Loss: 0.00013\n",
      "Epoch [100/10000], Loss: 0.00010\n",
      "stop boundary condition optimization\n"
     ]
    }
   ],
   "source": [
    "MAX = 10000\n",
    "for epoch in range(MAX):\n",
    "    u_pred_bc = u_net(x_batch,t_batch)\n",
    "    # loss\n",
    "    loss_bc = torch.mean( (u_pred_bc)**2 )\n",
    "    # стираем предыдущий граф градиента\n",
    "    optimizer_bc.zero_grad()\n",
    "    # обратное распространение ошибки\n",
    "    loss_bc.backward()\n",
    "    # записываем новые коэфициента\n",
    "    optimizer_bc.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch [{epoch}/{MAX}], Loss: {loss_bc.item():.5f}')\n",
    "    if loss_bc < 0.0001:\n",
    "        print('stop boundary condition optimization')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "85cdc061-8304-458d-9a8a-a7a0ed8d0072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0205],\n",
       "        [-0.0167],\n",
       "        [-0.0127],\n",
       "        [-0.0084],\n",
       "        [-0.0039],\n",
       "        [ 0.0009],\n",
       "        [ 0.0060],\n",
       "        [ 0.0113],\n",
       "        [ 0.0169],\n",
       "        [ 0.0226],\n",
       "        [ 0.0032],\n",
       "        [ 0.0027],\n",
       "        [ 0.0022],\n",
       "        [ 0.0016],\n",
       "        [ 0.0010],\n",
       "        [ 0.0003],\n",
       "        [-0.0005],\n",
       "        [-0.0013],\n",
       "        [-0.0022],\n",
       "        [-0.0032]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net(x_batch,t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12c3d6-6a6e-4ec3-97b5-0ddc20414ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf8f5c-efd5-4c6f-af0a-d3067a505a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
